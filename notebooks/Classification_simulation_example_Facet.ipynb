{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "raw_mimetype": "text/html"
            },
            "source": [
                "<img src=\"../_static/Gamma_Facet_Logo_RGB_LB.svg\" width=\"500\" style=\"padding-bottom: 70px; padding-top: 70px; margin: auto; display: block\">"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Classification with FACET\n",
                "\n",
                "***\n",
                "\n",
                "**Robust and impactful Data Science with FACET**\n",
                "\n",
                "FACET enables us to perform a number of critical steps in best practice Data Science work flow easily, efficiently and reproducibly:\n",
                "\n",
                "1. Create a robust pipeline for learner selection using LearnerRanker and enabling the use of bootstrap cross-validation.\n",
                "\n",
                "2. Enhance our model inspection to understand drivers of predictions using local explanations of features via [SHAP values](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions) by applying a novel methodology that decomposes SHAP values into measures of synergy, redundancy, and independence between each pair of features.\n",
                "\n",
                "3. Quickly apply historical simulation to gain key insights into feature values that minimize or maximize the predicted outcome.\n",
                "\n",
                "***\n",
                "\n",
                "**Context**\n",
                "\n",
                "With the advaced capabilities FACET provides by extending SHAP-based model inspection, it is important to gain some intution for how the newly introduced measures for feature redundancy and synergy can vary. As SHAP values represent post-processing after data preparation, feature engineering, preprocessing and model selection/tuning, minimal simulation studies offer a way to make the connection as direct as possible."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In this FACET tutorial we will conduct two simulation studies to gain intuition about synergy and redundancy:\n",
                "\n",
                "- explore patterns in synergy and redundancy as a function of the individual and joint contribution of two continuous features in predicting a binary target where the features have varying degrees of correlation.\n",
                "- explore synergy and redundancy as a function of regularization of a random forest by varying the `max_depth` parameter.\n",
                "\n",
                "***\n",
                "\n",
                "**Tutorial outline**\n",
                "\n",
                "1. [Redundancy and Synergy](#Redundancy-and-Synergy)\n",
                "2. [Data simulation](#Data-simulation)\n",
                "3. [How redundancy and synergy change with feature correlation and interaction](#How-redundancy-and-synergy-change-with-feature-correlation-and-interaction)\n",
                "4. [How redundancy and synergy change with regularization in a Random Forest](#How-redundancy-and-synergy-change-with-regularization-in-a-Random-Forest)\n",
                "5. [Summary](#Summary)\n",
                "6. [What can you do next?](#What-can-you-do-next?)\n",
                "7. [Appendix](#Appendix)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# standard imports\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from scipy.linalg import toeplitz\n",
                "import matplotlib.pyplot as plt\n",
                "import shap\n",
                "import itertools\n",
                "import seaborn as sns\n",
                "\n",
                "# FACET imports\n",
                "from facet import Sample\n",
                "from facet.crossfit import LearnerCrossfit\n",
                "from facet.inspection import LearnerInspector\n",
                "from facet.selection import LearnerRanker, LearnerGrid\n",
                "from facet.validation import BootstrapCV\n",
                "\n",
                "# sklearndf imports\n",
                "from sklearndf.pipeline import PipelineDF, ClassifierPipelineDF\n",
                "from sklearndf.classification import RandomForestClassifierDF"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Redundancy, Synergy and SHAP\n",
                "\n",
                "Redundancy and synergy are part of the key extensions FACET makes to using SHAP values to understand model predictions. \n",
                "\n",
                "The [SHAP approach](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions) has become the standard method for model inspection. SHAP values are used to explain the additive contribution of each feature to the prediction for a given observation. SHAP values are computed for every feature and observation.\n",
                "\n",
                "The FACET `LearnerInspector` computes SHAP values for each crossfit (i.e., a CV fold or bootstrap resample) using the best model identified by the `LearnerRanker`. The FACET `LearnerInspector` then provides advanced model inspection through new SHAP-based summary metrics for understanding feature redundancy and synergy.\n",
                "\n",
                "The definitions are as follows:\n",
                "\n",
                "1. **Redundancy** represents how much information is shared between two features contributions to model predictions. In our example we might expect redundancy between BMI and waist circumference as a higher BMI will tend to lead to a larger waist circumference and vice versa. This means just knowing one or the other is likely to provide similar predictive performance.\n",
                "\n",
                "\n",
                "2. **Synergy** represents how much the combined information of two features contributes to the model predictions. In our example we could hypothesize that knowing both gender and BMI provides greater accuracy in predicting prediabetes risk than either alone.\n",
                "\n",
                "In breif, redundancy represents the shared information between two features and synergy represents the degree to which one feature combines with another to generate a prediction. It is also important to recognize:\n",
                "\n",
                "- that any pair of features may have both redundancy and synergy\n",
                "- that SHAP values are dependent upon the model, so under- or over-fitting will influence redundancy and synergy"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Data simulation\n",
                "\n",
                "For the simulation studies we generate data as follows:"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "$$ (X_1, X_2) \\sim N\\left[\\left(\\begin{array}{c} 0\\\\0 \\end{array}\\right), \\left(\\begin{array}{cc} 1 & \\rho\\\\ \\rho & 1 \\end{array}\\right)\\right]$$\n",
                "   \n",
                "    \n",
                "$$p = \\cfrac{1}{1 + exp(-[\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2])}$$\n",
                "    \n",
                "    \n",
                "$$U \\sim \\textrm{U}(0,1)$$\n",
                "    \n",
                "    \n",
                "$$y = \\begin{cases}\n",
                "1 & U < p  \\\\\n",
                "0 & U \\geq p\n",
                "\\end{cases}$$"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Importantly we use the correlation ($\\rho$) between features as a way to induce redundancy, and the balance between an interaction ($\\beta_3$) and main effects ($\\beta_1, \\beta_2$) to induce synergy. So for example, as the correlation gets higher we expect higher redundancy, and as the interaction gets stronger and the main effects get weaker, we expect higher synergy.\n",
                "\n",
                "The function used to simulate data accoring to the above specifications is `sim_interaction()` and can be found in the [Appendix](#Data-simulation-code)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# How redundancy and synergy change with feature correlation and interaction"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "For this first simulation case study, we use the following parameters for data generation:\n",
                "\n",
                "- intercept ($\\beta_0$) = `[0]`\n",
                "- main effects ($\\beta_1, \\beta_2$) = `[0, 1, 2, 3]`\n",
                "- interaction ($\\beta_3$) = `[1, 2, 3]`\n",
                "- correlation ($\\rho$) = `[0, 0.2, 0.4, 0.6, 0.8]`\n",
                "\n",
                "For each combination of parameters above we simulate 20 datasets with 2000 observations. Model fitting is performed using 10 replicates of Bootstrap CV. The classifier used is a Random Forest with default hyper-parameters.\n",
                "\n",
                "The code used to generate the data presented is shown in the [Appendix](#Simulation-study-1-code). You can experiment with the code and perform your own simulation studies, just be aware that it may take some time to run."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load simulation study data\n",
                "sns.set_style(\"darkgrid\")\n",
                "sim_data = pd.read_csv('sphinx/source/tutorial/classification_sim1.csv').set_index(['main effects', 'interaction', 'correlation'])\n",
                "long_sim = sim_data[['redundancy', 'synergy']].stack().reset_index()\n",
                "long_sim.rename(columns={'level_3':'FACET metric', 0:'Redundany / Synergy'}, inplace=True)\n",
                "\n",
                "# create summary plot\n",
                "sns.set_palette([\"#30c1d7\", \"#295e7e\"])\n",
                "g = sns.FacetGrid(long_sim, col=\"main effects\", row=\"interaction\", hue='FACET metric', margin_titles=True)\n",
                "g.map(sns.lineplot, \"correlation\", \"Redundany / Synergy\", estimator='mean', marker=\"o\", ci=None)\n",
                "g.add_legend()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "There are several interesting patterns we can observe from the plot:\n",
                "\n",
                "1. In general the larger the interaction the higher the synergy.\n",
                "2. The greater the individual contributions of the features the lower the synergy.\n",
                "3. As the correlation increases they redundancy increases and the synergy reduces.\n",
                "4. Redundancy increases with increasing individual contributions of the features."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# How redundancy and synergy change with regularization in a Random Forest\n",
                "\n",
                "In this second simulation study we are going to explore the values of Synergy and Redundancy as a function of regularization in a random forest. The main regularization parameter is the `max_depth`, which controls the tree depth. In general, the deeper the tree the more likely we are to overfit the data.\n",
                "\n",
                "For this second simulation case study, we use the following parameters for data generation:\n",
                "\n",
                "- intercept ($\\beta_0$) = `[0]`\n",
                "- main effects ($\\beta_1, \\beta_2$) = `[1]`\n",
                "- interaction ($\\beta_3$) = `[3]`\n",
                "- correlation ($\\rho$) = `[0.5]`\n",
                "\n",
                "We simulate 20 datasets with 1000 observations. The classifier used is a Random Forest with default hyper-parameters, except as follows:\n",
                "\n",
                "- `max_depth = [2, 4, 8, 16, 32]`\n",
                "- `n_estimators = [250]`\n",
                "\n",
                "For each combination of parameters above we perform model fitting using 10 replicates of Bootstrap CV for each of the 20 simulated datasets.\n",
                "\n",
                "The code used to generate the data presented is shown in the [Appendix](#Simulation-study-2-code). You can experiment with that code and perform your own simulation studies, just be aware that it may take some time to run."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# load simulation study data\n",
                "sns.set_style(\"darkgrid\")\n",
                "sim_data = pd.read_csv('sphinx/source/tutorial/classification_sim2.csv').set_index(['max_depth', 'n_estimators'])\n",
                "long_sim = sim_data[['redundancy', 'synergy']].stack().reset_index()\n",
                "long_sim.rename(columns={'level_2':'FACET metric', 0:'Redundany / Synergy'}, inplace=True)\n",
                "\n",
                "# create summary plot\n",
                "sns.set_palette([\"#30c1d7\", \"#295e7e\"])\n",
                "sns.lineplot(\"max_depth\", \"Redundany / Synergy\", data=long_sim, hue='FACET metric', estimator='mean', marker=\"o\", ci=None)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can observe the following from the figure above:\n",
                "\n",
                "1. As `max_depth` increased so did synergy. Synergy changed from an initial value of 0.30 with a `max_depth` of 2, and increased to 0.55 (almost doubling) by the time we reach a `max_depth` of 32.\n",
                "2. As `max_depth` increased, redundancy decreased. Redundancy changed from an initial value of 0.27 with a `max_depth` of 2, and decreased to 0.22 by the time we reach a `max_depth` of 32.\n",
                "3. This suggests for a pair of moderately correlated features with a moderate interaction and limited individual contributions, overfitting might cause us to over-estimate synergy and under-estimate redundnancy. "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Summary\n",
                "\n",
                "We conducted two simulation studies using a simple controlled setting where we knew the amount of correlation, individual and combined contributions to a binary target.\n",
                "\n",
                "- In the first simulation study we saw that the amount of correlation between two features as well as the strength of interaction and degree of independent contributions drives the balance between synergy and redudnacy. \n",
                "- In the second simulation study we saw how both synergy and redundancy changed as a function of the `max_depth` parameter of our Random Forest classifier. In particular, for a pari of features with correlation and interaction, how as `max_depth` increased synery increased and redundancy decreased."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# What can you do next?"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "There are a number of next steps that could be taken to gain further intuition regarding the capabilities of FACET:\n",
                "    \n",
                "1. Explore further values of main-effects, interactions and correlation between the two features used in the simulation studies.\n",
                "2. Add further features to the simulation and explore what happends when you have features that are correlated but only one contributes to prediction (i.e., a purely redundant feature).\n",
                "3. Try different learners and hyper-parameters and see how the redundancy and synergy results change. Remember, the contributions of features to individual predictions is through the \"eyes\" of the model."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Appendix"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data simulation code"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```\n",
                "def sim_interaction(\n",
                "    n: int = 2000,\n",
                "    intercept: float = None,\n",
                "    coef_1: float = None,\n",
                "    coef_2: float = None,\n",
                "    coef_3: float = None,\n",
                "    corr: float = 0\n",
                "):\n",
                "\n",
                "    # two standard normal features for interaction term in the linear predictor\n",
                "    # note np function for MVN takes sigma but as we use standard normal \n",
                "    corr = np.array([[1, corr], [corr, 1]])\n",
                "    mu = [0, 0]\n",
                "    tmp_data = pd.DataFrame(\n",
                "        np.random.multivariate_normal(mu, corr, size=n),\n",
                "        columns=[\"feature_1\", \"feature_2\"],\n",
                "    )\n",
                "\n",
                "    # linear predictor\n",
                "    lp = (\n",
                "        intercept\n",
                "        + coef_1 * tmp_data.feature_1\n",
                "        + coef_2 * tmp_data.feature_2\n",
                "        + coef_3 * tmp_data.feature_1 * tmp_data.feature_2\n",
                "    )\n",
                "\n",
                "    # convert to probability\n",
                "    prob = 1 / (1 + np.exp(-lp))\n",
                "\n",
                "    # create target\n",
                "    tmp_data[\"target\"] = np.where(prob <= np.random.uniform(size=n), 0, 1)\n",
                "\n",
                "    return tmp_data\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Simulation study 1 code"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```\n",
                "\n",
                "# conditions to iterate over\n",
                "main_effects = [0, 1, 2, 3]\n",
                "interaction = [1, 2, 3]\n",
                "corr = [0, 0.2, 0.4, 0.6, 0.8]\n",
                "conditions = list(itertools.product(*[main_effects, interaction, corr]))\n",
                "n_sims = 20\n",
                "n_conditions = len(conditions)\n",
                "full_results = pd.DataFrame([])\n",
                "\n",
                "# iterate over conditions\n",
                "for i in range(n_conditions):\n",
                "    \n",
                "    # number of iterations for a set of conditions\n",
                "    for j in range(n_sims):\n",
                "        \n",
                "        # simulate data\n",
                "        sim_df = sim_interaction(intercept=0,\n",
                "                                 coef_1=conditions[i][0],\n",
                "                                 coef_2=conditions[i][0],\n",
                "                                 coef_3=conditions[i][1],\n",
                "                                 corr=conditions[i][2])\n",
                "        \n",
                "        # run crossfit\n",
                "        crossfit = LearnerCrossfit(\n",
                "            pipeline=ClassifierPipelineDF(classifier=RandomForestClassifierDF(random_state=42)),\n",
                "            cv=BootstrapCV(n_splits=10, random_state=42),\n",
                "            n_jobs=-1,\n",
                "        ).fit(sample = Sample(\n",
                "            observations=sim_df,\n",
                "            features=['feature_1', 'feature_2'],\n",
                "            target='target'\n",
                "        ))\n",
                "\n",
                "        # do a straight crossfit with fit inspector\n",
                "        inspector = LearnerInspector(n_jobs=-1).fit(crossfit=crossfit)\n",
                "\n",
                "        # obtain synergy and redundancy\n",
                "        redundancy_matrix = inspector.feature_redundancy_matrix()\n",
                "        synergy_matrix = inspector.feature_synergy_matrix()\n",
                "        \n",
                "        # assemble results\n",
                "        tmp_results = pd.Series({'coef_1': conditions[i][0],\n",
                "                   'coef_2': conditions[i][0],\n",
                "                   'interaction': conditions[i][1],\n",
                "                   'correlation': conditions[i][2],\n",
                "                   'redundancy': redundancy_matrix.loc['feature_1', 'feature_2'],\n",
                "                   'synergy': synergy_matrix.loc['feature_1', 'feature_2'],\n",
                "                   'y_mean': sim_df.target.mean()})\n",
                "        \n",
                "        full_results = full_results.append(tmp_results, ignore_index=True)\n",
                "        \n",
                "full_results['main effects'] = \"(\" + full_results['coef_1'].astype(str) + \", \" + full_results['coef_2'].astype(str) + \")\"\n",
                "\n",
                "# output to a csv file - and use in generating notebook\n",
                "full_results.to_csv('sphinx/source/tutorial/classification_sim1.csv', index=False)\n",
                "\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Simulation study 2 code"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "```\n",
                "max_depth = [2, 4, 8, 16, 32]\n",
                "n_estimators = [250]\n",
                "parm_grid = list(itertools.product(*[max_depth, n_estimators]))\n",
                "n_sims = 20\n",
                "n_parms = len(parm_grid)\n",
                "full_results = pd.DataFrame([])\n",
                "\n",
                "# number of iterations for a set of conditions\n",
                "for j in range(n_sims):\n",
                "        \n",
                "    # simulate data\n",
                "    sim_df = sim_interaction(n=1000,\n",
                "                             intercept=0,\n",
                "                             coef_1=1,\n",
                "                             coef_2=1,\n",
                "                             coef_3=3,\n",
                "                             corr=0.5)\n",
                "    \n",
                "    # split into train and test so we can estimate test error as well\n",
                "    print(j)\n",
                "    \n",
                "    # iterate over hyper-parameters\n",
                "    for i in range(n_parms):\n",
                "        \n",
                "        # create classifier with required hyper-parameters\n",
                "        clf = ClassifierPipelineDF(\n",
                "            classifier=RandomForestClassifierDF(\n",
                "                random_state=42,\n",
                "                max_depth=parm_grid[i][0],\n",
                "                n_estimators=parm_grid[i][1]\n",
                "            )\n",
                "        )\n",
                "        \n",
                "        # run crossfit\n",
                "        crossfit = LearnerCrossfit(\n",
                "            pipeline=clf,\n",
                "            cv=BootstrapCV(n_splits=10, random_state=42),\n",
                "            n_jobs=-1,\n",
                "        ).fit(sample = Sample(\n",
                "            observations=sim_df,\n",
                "            features=['feature_1', 'feature_2'],\n",
                "            target='target'\n",
                "        ))\n",
                "\n",
                "        # do a straight crossfit with fit inspector\n",
                "        inspector = LearnerInspector(n_jobs=-1).fit(crossfit=crossfit)\n",
                "\n",
                "        # obtain synergy and redundancy\n",
                "        redundancy_matrix = inspector.feature_redundancy_matrix()\n",
                "        synergy_matrix = inspector.feature_synergy_matrix()\n",
                "        \n",
                "        # assemble results\n",
                "        tmp_results = pd.Series({'max_depth': parm_grid[i][0],\n",
                "                   'n_estimators': parm_grid[i][1],\n",
                "                   'redundancy': redundancy_matrix.loc['feature_1', 'feature_2'],\n",
                "                   'synergy': synergy_matrix.loc['feature_1', 'feature_2'],\n",
                "                   'y_mean': sim_df.target.mean()})\n",
                "        \n",
                "        full_results = full_results.append(tmp_results, ignore_index=True)\n",
                "        \n",
                "# output to a csv file - and use in generating notebook\n",
                "full_results.to_csv('sphinx/source/tutorial/classification_sim2.csv', index=False)\n",
                "```"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.8"
        },
        "toc": {
            "base_numbering": 1,
            "nav_menu": {},
            "number_sections": true,
            "sideBar": true,
            "skip_h1_title": false,
            "title_cell": "Table of Contents",
            "title_sidebar": "Contents",
            "toc_cell": false,
            "toc_position": {},
            "toc_section_display": true,
            "toc_window_display": false
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}