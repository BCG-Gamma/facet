{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<img src=\"../_static/Gamma_Facet_Logo_RGB_LB.svg\" width=\"500\" style=\"padding-bottom: 70px; padding-top: 70px; margin: auto; display: block\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to FACET\n",
    "\n",
    "***\n",
    "\n",
    "**Robust and impactful Data Science with FACET**\n",
    "\n",
    "FACET enables us to perform several critical steps in best practice Data Science work flow easily, efficiently and reproducibly:\n",
    "\n",
    "1. Create a robust pipeline for learner selection using LearnerRanker and cross-validation.\n",
    "\n",
    "2. Enhance our model inspection to understand drivers of predictions using local explanations of features via [SHAP values](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions) by applying a novel methodology that decomposes SHAP values into measures of synergy, redundancy, and independence between each pair of features.\n",
    "\n",
    "3. Quickly apply historical simulation to gain key insights into feature values that minimize or maximize the predicted outcome.\n",
    "\n",
    "***\n",
    "\n",
    "**Context**\n",
    "\n",
    "Drilling a water well is dangerous and costly. Costs are driven by the time it takes to finalize a well in order to start pumping water from it. In order to reduce those costs, drillers are usually incentivised to drill at a faster pace. However, drilling faster increases risks of incident which is the reason why the Rate of Penetration (ROP) is a measure constantly monitored.\n",
    "\n",
    "Utilizing FACET, we will:\n",
    "\n",
    "1. Apply use machine learning to prevent a water well drilling operation from an incident.\n",
    "2. Quantify how the ROP impacts the estimated risk.  \n",
    "\n",
    "***\n",
    "\n",
    "**Tutorial outline**\n",
    "\n",
    "1. [Required imports](#Required-imports)\n",
    "2. [Data and initial feature selection](#Data-and-initial-feature-selection)\n",
    "3. [Selecting a learner using FACET ranker](#Selecting-a-learner-using-FACET-ranker)\n",
    "4. [Using the FACET inspector for model inspection](#Using-the-FACET-inspector-for-model-inspection)\n",
    "5. [FACET univariate simulator: the impact of rate of penetration](#FACET-univariate-simulator:-the-impact-of-rate-of-penetration)\n",
    "6. [Appendix](#Appendix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T08:33:28.594834Z",
     "start_time": "2020-08-31T08:33:28.410363Z"
    },
    "delete_for_interactive": true,
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# this cell's metadata contains\n",
    "# \"nbsphinx\": \"hidden\" so it is hidden by nbsphinx\n",
    "\n",
    "\n",
    "def _set_paths() -> None:\n",
    "    # set the correct path when launched from within PyCharm\n",
    "\n",
    "    module_paths = [\"pytools\", \"facet\", \"sklearndf\"]\n",
    "\n",
    "    import sys\n",
    "    import os\n",
    "\n",
    "    if \"cwd\" not in globals():\n",
    "        # noinspection PyGlobalUndefined\n",
    "        global cwd\n",
    "        cwd = os.path.join(os.getcwd(), os.pardir, os.pardir, os.pardir)\n",
    "        os.chdir(cwd)\n",
    "    print(f\"working dir is '{os.getcwd()}'\")\n",
    "    for module_path in module_paths:\n",
    "        if module_path not in sys.path:\n",
    "            sys.path.insert(0, os.path.abspath(f\"{cwd}/{os.pardir}/{module_path}/src\"))\n",
    "        print(f\"added `{sys.path[0]}` to python paths\")\n",
    "\n",
    "\n",
    "def _ignore_warnings():\n",
    "    # ignore irrelevant warnings that would affect the output of this tutorial notebook\n",
    "\n",
    "    # ignore a useless LGBM warning\n",
    "    import warnings\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, message=r\".*Xcode_8\\.3\\.3\")\n",
    "\n",
    "\n",
    "_set_paths()\n",
    "_ignore_warnings()\n",
    "\n",
    "del _set_paths, _ignore_warnings\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def _configure_matplotlib():\n",
    "    # set global options for matplotlib\n",
    "\n",
    "    import matplotlib\n",
    "\n",
    "    matplotlib.rcParams[\"figure.figsize\"] = (16.0, 8.0)\n",
    "    matplotlib.rcParams[\"figure.dpi\"] = 72\n",
    "\n",
    "\n",
    "_configure_matplotlib()\n",
    "\n",
    "del _configure_matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-28T17:21:45.452088Z",
     "start_time": "2020-08-28T17:21:45.450036Z"
    }
   },
   "source": [
    "# Required imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run this notebook, we will import not only the FACET package, but also other packages useful to solve this task. Overall, we can break down the imports into three categories: \n",
    "\n",
    "1. Common packages (pandas, matplotlib, etc.)\n",
    "2. Required FACET classes (inspection, selection, validation, simulation, etc.)\n",
    "3. Other BCG Gamma packages which simplify pipelining (sklearndf, see on [GitHub](https://github.com/BCG-Gamma/sklearndf/)) and support visualization (pytools, see on [GitHub](https://github.com/BCG-Gamma/pytools)) when using FACET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-28T17:21:38.623408Z",
     "start_time": "2020-08-28T17:21:38.620085Z"
    }
   },
   "source": [
    "**Common package imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T08:34:10.269188Z",
     "start_time": "2020-08-31T08:34:02.013Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Gamma FACET imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T08:33:30.464262Z",
     "start_time": "2020-08-31T08:33:30.101989Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from facet.data import Sample\n",
    "from facet.inspection import LearnerInspector\n",
    "from facet.selection import LearnerRanker, LearnerGrid\n",
    "from facet.validation import BootstrapCV\n",
    "from facet.simulation.partition import ContinuousRangePartitioner\n",
    "from facet.simulation import UnivariateProbabilitySimulator\n",
    "from facet.simulation.viz import SimulationDrawer\n",
    "from facet.crossfit import LearnerCrossfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T14:15:20.686543Z",
     "start_time": "2020-08-18T14:15:20.683573Z"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**sklearndf imports**\n",
    "\n",
    "Instead of using the \"regular\" scikit-learn package, we are going to use sklearndf (see on [GitHub](https://github.com/BCG-Gamma/sklearndf/)). sklearndf is an open source library designed to address a common issue with scikit-learn: the outputs of transformers are numpy arrays, even when the input is a data frame. However, to inspect a model it is essential to keep track of the feature names. sklearndf retains all the functionality available through scikit-learn plus the feature traceability and usability associated with Pandas data frames. Additionally, the names of all your favourite scikit-learn functions are the same except for `DF` on the end. For example, the standard scikit-learn import:\n",
    "\n",
    "`from sklearn.pipeline import Pipeline`\n",
    "\n",
    "becomes:\n",
    "\n",
    "`from sklearndf.pipeline import PipelineDF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T08:33:30.620441Z",
     "start_time": "2020-08-31T08:33:30.465741Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearndf.pipeline import PipelineDF, ClassifierPipelineDF\n",
    "from sklearndf.classification import RandomForestClassifierDF\n",
    "from sklearndf.classification.extra import LGBMClassifierDF\n",
    "from sklearndf.transformation.extra import BorutaDF\n",
    "from sklearndf.transformation import SimpleImputerDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**pytools imports**\n",
    "\n",
    "pytools (see on [GitHub](https://github.com/BCG-Gamma/pytools)) is an open source library containing general machine learning and visualization utilities, some of which are useful for visualising the advanced model inspection capabilities of FACET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T08:33:30.629194Z",
     "start_time": "2020-08-31T08:33:30.622223Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pytools.viz.dendrogram import DendrogramDrawer, DendrogramReportStyle\n",
    "from pytools.viz.distribution import ECDFDrawer\n",
    "from pytools.viz.matrix import MatrixDrawer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data and initial feature selection\n",
    "\n",
    "For the sake of simplicity, we use a simplified artificial dataset, it contains 500 observations, each row representing a drilling operation of the past, the target is the occurrence of drill breakdown (incident). Details and the code used to simulate this dataset can be found in the [Appendix](#Appendix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T08:33:30.656685Z",
     "start_time": "2020-08-31T08:33:30.639398Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the prepared dataframe\n",
    "df = pd.read_csv(\n",
    "    \"sphinx/source/tutorial/water_drill_data_classification.csv\",\n",
    "    sep=\";\",\n",
    "    encoding=\"utf-8\",\n",
    ")\n",
    "\n",
    "# quick look\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T08:33:30.665214Z",
     "start_time": "2020-08-31T08:33:30.659809Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create a FACET sample object\n",
    "drilling_obs = Sample(observations=df, target_name=\"Incident\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we perform some initial feature selection using Boruta, a recent approach shown to have quite good performance. The Boruta algorithm removes features that are no more predictive than random noise. If you are interested further, please see this  [article](https://www.jstatsoft.org/article/view/v036i11).\n",
    "\n",
    "The `BorutaDF` transformer in our sklearndf package provides easy access to this powerful method. The approach relies on a tree-based learner, usually a random forest. For settings, a `max_depth` of between 3 and 7 is typically recommended, and here we rely on the default setting of 5. However, as this depends on the number of features and the complexity of interactions one could also explore the sensitivity of feature selection to this parameter. The number of trees is automatically managed by the Boruta feature selector argument `n_estimators=\"auto\"`.\n",
    "\n",
    "We also use parallelization for the random forest using `n_jobs` to accelerate the Boruta iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T08:53:50.515286Z",
     "start_time": "2020-08-31T08:53:26.621355Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# wrapper class to implement Boruta feature selection\n",
    "feature_selector = BorutaDF(\n",
    "    estimator=RandomForestClassifierDF(max_depth=5, random_state=42, n_jobs=-3),\n",
    "    n_estimators=\"auto\",\n",
    "    random_state=42,\n",
    "    verbose=0,\n",
    "    max_iter=200,\n",
    ")\n",
    "\n",
    "# create a pipeline that includes some simple preprocessing (imputation) and Boruta\n",
    "feature_preprocessing = PipelineDF(\n",
    "    steps=[(\"impute\", SimpleImputerDF()), (\"feature selection\", feature_selector)]\n",
    ")\n",
    "\n",
    "# run feature selection using Boruta and report those selected\n",
    "feature_preprocessing.fit(X=drilling_obs.features, y=drilling_obs.target)\n",
    "print(f\"Selected features: {list(feature_preprocessing.feature_names_original_.unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can see that the key features that we would expect to impact the safety of the operation are included after the feature selection. A working hypothesis of how each influences the target is: \n",
    "\n",
    "- **Weight on bit**: we expect higher weight to increase the likelihood of a failure due to heavier equipment wear\n",
    "\n",
    "- **Rotation speed**: Too fast rotation speed can lead to overheating and breaking the material, too low rotation renders drilling more difficult and is not economical\n",
    "\n",
    "- **Depth of operation**: As a simplification we will take for granted that the deeper we dig, the denser the soil will be, increasing the likelihood of either a collapse or breaking equipment wear \n",
    "\n",
    "- **Hole diameter**: Thinner wholes are used in deeper sections of the well hence usually relate to more dangerous zones\n",
    "\n",
    "- **Rate of Penetration**: A higher ROP leads to more wear & tear of the equipment and thus we expect a positive effect\n",
    "\n",
    "- **Inverse Rate of Penetration**: As described by its name, this feature is the inverse of the ROP\n",
    "\n",
    "- **Mud density**: Mud density needs to match soil density to avoid well collapse (formation falling in well and blocking pipe) or mud loss (mud flowing in the formation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_preprocessing.feature_names_original_.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create a FACET sample object with features selected by Boruta\n",
    "drilling_obs_reduced_featset = drilling_obs.keep(feature_names=\n",
    "    feature_preprocessing.feature_names_original_.unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T10:53:26.316Z"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Selecting a learner using FACET ranker\n",
    "\n",
    "FACET implements several additional useful wrappers which further simplify comparing and tuning a larger number of models and configurations: \n",
    "\n",
    "- `LearnerGrid`: allows you to pass a learner pipeline (i.e., classifier + any preprocessing) and a set of hyperparameters\n",
    "- `LearnerRanker`: multiple LearnerGrids can be passed into this class as a list - this allows tuning hyperparameters both across different types of learners in a single step and ranks the resulting models accordingly\n",
    "\n",
    "The following learners and hyperparameter ranges will be assessed using 5 repeated 5-fold cross-validation:\n",
    "\n",
    "\n",
    "1. **Random forest**: with hyperparameters\n",
    "    - min_samples_leaf: [8, 11, 15]\n",
    "\n",
    "  \n",
    "2. **Light gradient boosting**: with hyperparameters\n",
    "    - min_samples_leaf: [8, 11, 15]\n",
    "\n",
    "Note if you want to see a list of hyperparameters you can use `classifier_name().get_params().keys()` where `classifier_name` could be for example `RandomForestClassifierDF` and if you want to see the default values, just use `classifier_name().get_params()`.\n",
    "\n",
    "Note that ranking uses the average performance minus two times the standard deviation, so that we consider both the average performance and variability when selecting a classifier. The default scoring metric for classification is accuracy.\n",
    "\n",
    "First, we specify the classifiers we want to train using `ClassifierPipelineDF` from sklearndf. Note here we also include feature preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T08:33:54.961942Z",
     "start_time": "2020-08-31T08:33:54.958552Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# random forest learner\n",
    "rforest_clf = ClassifierPipelineDF(\n",
    "    classifier=RandomForestClassifierDF(n_estimators=500, random_state=42),\n",
    ")\n",
    "\n",
    "# light gradient boosting learner\n",
    "lgbm_clf = ClassifierPipelineDF(\n",
    "    classifier=LGBMClassifierDF(random_state=42),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Then we create a list of learner grids where each learner grid is created using `LearnerGrid` and allows us to associate a `ClassifierPipelineDF` with a specified set of hyperparameter via the `learner_parameters` argument. Note this structure allows us to easily include additional classifiers and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define learner grid\n",
    "clf_grid = [\n",
    "    LearnerGrid(\n",
    "        pipeline=rforest_clf, learner_parameters={\"min_samples_leaf\": [8, 11, 15]}\n",
    "    ),\n",
    "    LearnerGrid(\n",
    "        pipeline=lgbm_clf, learner_parameters={\"min_data_in_leaf\": [8, 11, 15]}\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We now fit the grid defined above using the `LeanerRanker`, which will run a gridsearch (or random search if defined) using 5 repeated 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T08:34:04.796507Z",
     "start_time": "2020-08-31T08:33:54.964092Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create cv iterator 5 repeated 5-fold\n",
    "cv_approach = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "\n",
    "# fit ranker\n",
    "model_ranker = LearnerRanker(grids=clf_grid, cv=cv_approach, n_jobs=-3).fit(\n",
    "    sample=drilling_obs_reduced_featset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To see the configuration of the best selected model, we can access the `best_model_` property of the fitted `LearnerRanker` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T08:34:04.806643Z",
     "start_time": "2020-08-31T08:34:04.799183Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_ranker.best_model_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can see how each model scored using the `summary_report()` method of the `LearnerRanker`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T08:34:04.813316Z",
     "start_time": "2020-08-31T08:34:04.809195Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# let's look at performance for the top ranked classifiers\n",
    "model_ranker.summary_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T11:42:06.425585Z",
     "start_time": "2020-08-05T11:42:06.423740Z"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Using the FACET inspector for model inspection\n",
    "\n",
    "The [SHAP approach](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions) has become the standard method for model inspection. SHAP values are used to explain the additive contribution of each feature to the prediction for a given observation. SHAP values are computed for every feature and observation.\n",
    "\n",
    "The FACET `LearnerInspector` computes SHAP values for each crossfit (i.e., a CV fold or bootstrap resample) using the best model identified by the `LearnerRanker`. The FACET `LearnerInspector` then provides advanced model inspection through new SHAP-based summary metrics for understanding feature redundancy and synergy. Redundancy and synergy are calculated using the new algorithm FACET implements for using SHAP values to understand model predictions.\n",
    "\n",
    "The definitions are as follows:\n",
    "\n",
    "- **Redundancy** represents how much information is shared between two features contributions to the model predictions. For example, given features X and Y as coordinates on a chess board, the colour of a square can only be predicted when considering X and Y in combination. Redundancy is expressed as a percentage ranging from 0% (full uniqueness) to 100% (full redundancy).\n",
    "\n",
    "\n",
    "- **Synergy** represents how much the combined information of two features contributes to the model predictions. For example, temperature and pressure in a pressure cooker are redundant features for predicting cooking time since pressure will rise relative to the temperature, and vice versa. Therefore, knowing just one of either temperature or pressure will likely enable the same predictive accuracy. Synergy is expressed as a percentage ranging from 0% (full autonomy) to 100% (full synergy).\n",
    "\n",
    "Both cases can apply at the same time, i.e. a pair of features can use some information synergistically while using other information redundantly.\n",
    "\n",
    "To analyse redundancy for all possible feature parings, the approach is:\n",
    "\n",
    "1.\tCalculate the feature redundancy matrix using SHAP value decomposition - this gives us pairwise redundancy between features, in the range of 0.0 (fully unique contributions) and 1.0 (fully redundant contributions)\n",
    "2.\tTransform the feature redundancy matrix into a feature distance matrix, where distance is expressed as (1.0 - redundancy)\n",
    "3.\tPerform hierarchical, single-linkage clustering on the distance matrix, thus identifying groups of low-distance, redundant features which activate “in tandem” to predict the outcome\n",
    "\n",
    "The same approach can be used to analyse synergy.\n",
    "\n",
    "The inspector can calculate all of this with a single method call, but also offers methods to access the intermediate results of each step. A lightweight visualization framework is available to render the results in different styles.\n",
    "\n",
    "SHAP values from the `LearnerInspector` can also be used with the SHAP package plotting functions for sample and observation level SHAP visualizations, such as SHAP distribution plots, dependency plots, force plots and waterfall plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T08:34:09.495621Z",
     "start_time": "2020-08-31T08:34:04.816216Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_inspector = LearnerInspector(n_jobs=-3)\n",
    "model_inspector.fit(crossfit=model_ranker.best_model_crossfit_)\n",
    "\n",
    "redundancy_matrix = model_inspector.feature_redundancy_matrix()\n",
    "synergy_matrix = model_inspector.feature_synergy_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-11T15:12:46.059970Z",
     "start_time": "2020-08-11T15:12:46.055716Z"
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Feature redundancy**\n",
    "\n",
    "When plotting out the feature redundancy, we can see that there are some features which contain the same information to the model. In this case, these features are the depth of the operation and the hole diameter. \n",
    "Intuitively, we can see why these two features are redundant, as the depth of operation and the hole diameter are highly connected as drillers use thinner drilling bits as they drill deeper into the earth.\n",
    "\n",
    "As we don't want either of the features to confuse the model inference during the simulation step, we should **remove the hole diameter** for this example. \n",
    "We also observe an expected redundancy between ROP and Inverse Rate of Penetration (IROP), as the IROP is just the inverse of the ROP. We should **remove the IROP** as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MatrixDrawer(style=\"matplot%\").draw(redundancy_matrix, title=\"Redundancy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Feature synergy**\n",
    "\n",
    "When looking at the synergy matrix, we can easily figure out which of the features have an interaction effect on the target. We see that the **weight on the bit and the rotation speed in combination appear to have a high synergy.** Those two features are also synergistic with the ROP, which makes sense as the ROP is a consequence of the weight on bit and rotation speed that is applied.\n",
    "\n",
    "In hindsight, this appears obvious - drilling with both high bit weight and a high pace can have a disproportionately large impact on the wear of the equipment, thus drastically to higher failure likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-31T09:29:19.652252Z",
     "start_time": "2020-08-31T09:29:11.212923Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MatrixDrawer(style=\"matplot%\").draw(synergy_matrix, title=\"Synergy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Removing redundant features**\n",
    "\n",
    "As the picture above shows a high redundancy level between the **ROP** and the **IROP**, both features compete in terms of feature importance. The dendrogram below shows that **ROP** should be favoured though to orthogonalise the feature set before simulation. \n",
    "**Hole diameter** and **Depth of Operation** share also a high level of redundancy but the latter seems to hold more feature importance hence it is the **hole diameter** that will be removed to orthogonalise the feature set before simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "redundancy = model_inspector.feature_redundancy_linkage()\n",
    "DendrogramDrawer().draw(title=\"Redundancy linkage\", data=redundancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In order to use the univariate simulator, we need to have an orthogonal set of features. This is needed such that artificially created samples stay plausible. Indeed, not removing the Inverse Rate of Penetration feature from the set would lead to unrealistic artificial observations while using the univariate simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove redundant features\n",
    "redundant_features = [\"Inverse Rate of Penetration (h/ft)\"]\n",
    "drilling_obs_not_redundant = drilling_obs_reduced_featset.drop(redundant_features)\n",
    "\n",
    "model_ranker = LearnerRanker(grids=clf_grid, cv=cv_approach, n_jobs=-3).fit(\n",
    "    sample=drilling_obs_not_redundant\n",
    ")\n",
    "\n",
    "model_inspector = LearnerInspector(n_jobs=-3)\n",
    "model_inspector.fit(crossfit=model_ranker.best_model_crossfit_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "redundancy = model_inspector.feature_redundancy_linkage()\n",
    "DendrogramDrawer().draw(title=\"Redundancy linkage\", data=redundancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that our feature set is looking more linearly independent, we can start making simulations to gain knowledge into how Rate of Penetration will impact failure likelihood.\n",
    "\n",
    "Note that removing the IROP has given more feature importance to the ROP, now being the most important feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "synergy_matrix = model_inspector.feature_synergy_matrix()\n",
    "MatrixDrawer(style=\"matplot%\").draw(synergy_matrix, title=\"Synergy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in our revised synergy matrix we observe a 20% synergy between ROP and the rotation speed feature, which suggests an interaction between both variables that is not expressed in our dataset. As such, some of the reproduced artificial examples may break this interaction when performing univariate simulation for ROP which may slightly impact accuracy. As the interaction is relatively low and rotation speed remains a variable with strong predictive power, we will retain it in the feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# FACET univariate simulator: the impact of rate of penetration\n",
    "\n",
    "The ROP is a parameter very much monitored while drilling a well as it is a tradeoff between safety and economy, it is safer to drill at a low pace but much costlier as it takes more time. It has also the highest feature importance in our model (see dendrogram above). Let's use a simulation to get a sense of how the failure likelihood behaves if we simulate changes in the ROP applied.\n",
    "\n",
    "As the basis for the simulation, we divide the feature into relevant partitions: \n",
    "\n",
    "- We use FACET's `ContinuousRangePartitioner` to split the range of observed values of ROP into intervals of equal size. Each partition is represented by the central value of that partition. \n",
    "- For each partition, the simulator creates an artificial copy of the original sample assuming the variable to be simulated has the same value across all observations - which is the value representing the partition. Using the best `LearnerCrossfit` acquired from the ranker, the simulator now re-predicts all targets using the models trained for all folds and determines the average predicted probability of the target variable resulting from this.\n",
    "- The FACET `SimulationDrawer` allows us to visualise the result; both in a matplotlib and a plain-text style\n",
    "\n",
    "\n",
    "Finally, because FACET can use bootstrap cross validation, we can create a crossfit from our previous `LearnerRanker` best model to perform the simulation so we can quantify the uncertainty by using bootstrap confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create a bootstrap CV crossfit for simulation using best model\n",
    "boot_crossfit = LearnerCrossfit(\n",
    "    pipeline=model_ranker.best_model,\n",
    "    cv=BootstrapCV(n_splits=1000, random_state=42),\n",
    "    n_jobs=-3,\n",
    "    verbose=0,\n",
    ").fit(sample=drilling_obs_not_redundant)\n",
    "\n",
    "# set-up and run a simulation\n",
    "SIM_FEATURE = \"Rate of Penetration (ft/h)\"\n",
    "rop_bins = ContinuousRangePartitioner()\n",
    "rop_simulator = UnivariateProbabilitySimulator(crossfit=boot_crossfit, n_jobs=-3)\n",
    "rop_simulation = rop_simulator.simulate_feature(name=SIM_FEATURE, partitioner=rop_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facet.simulation.partition import IntegerRangePartitioner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rop_bins = IntegerRangePartitioner()\n",
    "rop_simulator = UnivariateProbabilitySimulator(crossfit=boot_crossfit, n_jobs=-3)\n",
    "rop_simulation = rop_simulator.simulate_feature(name=SIM_FEATURE, partitioner=rop_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SimulationDrawer().draw(data=rop_simulation, title=SIM_FEATURE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simulation can be used to obtain insight on failure likelihood changes depending on the ROP applied. As an example, the simulation suggests that operating with an ROP above 28ft/h can lead to an incident likelihood above 70%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "For the sake of simplicity, we use a simplified artificial dataset, it contains 500 observations, each row representing a drilling operation of the past, the target is the occurrence of drill breakdown (incident)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# additional imports\n",
    "from scipy.linalg import toeplitz\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional imports\n",
    "from scipy.linalg import toeplitz\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_sim(n=100,\n",
    "             intercept=-5,\n",
    "             linear_vars=10,\n",
    "             noise_vars=0,\n",
    "             corr_vars=0,\n",
    "             corr_type=\"AR1\",\n",
    "             corr_value=0,\n",
    "             mislabel=0,\n",
    "             surg_err=0.05,\n",
    "             bin_var_p=0,\n",
    "             bin_coef=0,\n",
    "             outcome=\"classification\",\n",
    "             regression_err=None,\n",
    "             drilling_ex=False\n",
    "             ):\n",
    "    \"\"\"\n",
    "    This function is for the most part a direct translation of the twoClassSim function from the R package caret.\n",
    "    Full credit for the approach used for simulating binary classification data foes to the Authors and contributors\n",
    "    of caret.\n",
    "\n",
    "    There are some modifications from the R implementation:\n",
    "    1. The ordinal outcome option has not been translated\n",
    "    2. The addition of another linear feature that is a copy of another used in the linear predictor with a small amount\n",
    "    of noise has been added to allow for the study of variable surrogacy\n",
    "    3. Option for a binary predictor and surrogate has also been added\n",
    "    4. Toggle option for regression versus classification has also been added\n",
    "\n",
    "    Source:\n",
    "        Caret: Kuhn, M. (2008). Caret package. Journal of Statistical Software, 28(5)\n",
    "        https://rdrr.io/cran/caret/man/twoClassSim.html\n",
    "\n",
    "    :param n: number of observations\n",
    "    :param intercept: value for the intercept which can be modified to generate class imbalance\n",
    "    :param linear_vars: number of linear features\n",
    "    :param noise_vars: number of noise features (i.e., do not contribute to the linear predictor)\n",
    "    :param corr_vars: number of correlated noise features\n",
    "    :param corr_type: type of correlation (exchangeable or auto-regressive) for correlated noise features\n",
    "    :param corr_value: correlation for correlated noise features\n",
    "    :param mislabel: proportion of mis-labelling of target if required\n",
    "    :param surg_err: degree of noise added to first linear predictor\n",
    "    :param bin_var_p: prevalence for a binary feature to include in linear predictor\n",
    "    :param bin_coef: coefficient for the impact of binary feature on linear predictor\n",
    "    :param outcome: can be either classification for a binary outcome or regression for a continuous outcome\n",
    "    :param regression_err: the error to be used in simulating a regression outcome\n",
    "    :param drilling_ex: flag to aplly specific modifications too the function\n",
    "    :return: data frame containing the simulated features and target for classification\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # set seed\n",
    "    np.random.seed(seed=4763546)\n",
    "\n",
    "    # add two correlated normal features\n",
    "    sigma = np.array([[1, 0], [0, 1]]) #(0s on the diagonal, 1.3 to 0, make the features independant) covariance matrix\n",
    "    mu = [0, 0]\n",
    "    tmp_data = pd.DataFrame(np.random.multivariate_normal(mu, sigma, size=n), columns=[\"TwoFactor1\", \"TwoFactor2\"])\n",
    "\n",
    "    # add linear features\n",
    "    if linear_vars > 0:\n",
    "        lin_cols = ['Linear' + str(x) for x in range(1, linear_vars + 1)]\n",
    "        tmp_data = pd.concat([tmp_data, pd.DataFrame(np.random.normal(size=(n, linear_vars)), columns=lin_cols)],\n",
    "                             axis=1)\n",
    "\n",
    "    # add features for non-linear terms\n",
    "    tmp_data['Nonlinear1'] = pd.Series(np.random.uniform(low=-1.0, high=1.0, size=n))\n",
    "    tmp_data = pd.concat([tmp_data, pd.DataFrame(np.random.uniform(size=(n, 2)), columns=['Nonlinear2', 'Nonlinear3'])],\n",
    "                         axis=1)\n",
    "\n",
    "    # add noise variables as needed\n",
    "    if noise_vars > 0:\n",
    "        noise_cols = ['Noise' + str(x) for x in range(1, noise_vars + 1)]\n",
    "        tmp_data = pd.concat([tmp_data, pd.DataFrame(np.random.normal(size=(n, noise_vars)), columns=noise_cols)],\n",
    "                             axis=1)\n",
    "\n",
    "    # add correlated noise features\n",
    "    if corr_vars > 0:\n",
    "        if corr_type == \"exch\":\n",
    "            vc = corr_value * np.ones((corr_vars, corr_vars))\n",
    "            np.fill_diagonal(vc, 1)\n",
    "\n",
    "        elif corr_type == \"AR1\":\n",
    "            vc_values = corr_value ** np.arange(corr_vars)\n",
    "            vc = toeplitz(vc_values)\n",
    "\n",
    "        corr_cols = ['Corr' + str(x) for x in range(1, corr_vars + 1)]\n",
    "        tmp_data = pd.concat([tmp_data,\n",
    "                              pd.DataFrame(np.random.multivariate_normal(np.zeros(corr_vars), vc, size=n),\n",
    "                                           columns=corr_cols)],\n",
    "                             axis=1)\n",
    "\n",
    "    # add a surrogate linear feature\n",
    "    if linear_vars > 0:\n",
    "        tmp_data['Linear1_prime'] = tmp_data['Linear1'] + np.random.normal(0, surg_err, size=n)\n",
    "\n",
    "    # add a binary feature\n",
    "    if bin_var_p > 0:\n",
    "        tmp_data['Binary1'] = np.where(np.random.uniform(size=n) <= bin_var_p, 0, 1)\n",
    "\n",
    "    # generate contributions to linear predictor 4, 4, 2 - 0,0,4 or 5 means features will have no correlations but contributions of the predictions will only have interaction \n",
    "    lp = intercept + 0 * tmp_data.TwoFactor1 + 0 * tmp_data.TwoFactor2 + 8 * tmp_data.TwoFactor1 * tmp_data.TwoFactor2 \\\n",
    "         + tmp_data.Nonlinear1 ** 3 + 2 * np.exp(-6 * (tmp_data.Nonlinear1 - 0.3) ** 2) + \\\n",
    "         2 * np.sin(np.pi * tmp_data.Nonlinear2 * tmp_data.Nonlinear3)\n",
    "        \n",
    "\n",
    "    if linear_vars > 0:\n",
    "        lin_coeff = np.linspace(10, 1, num=linear_vars) / 4\n",
    "        # Set some negative coefficients of linear relationship with lp\n",
    "        neg_idx = [_ for _ in range(1, linear_vars, 2)]\n",
    "        lin_coeff[neg_idx] = lin_coeff[neg_idx] * -1\n",
    "        if drilling_ex:\n",
    "            lin_coeff[2] = 4\n",
    "            lp = lp + tmp_data[lin_cols[2]]*tmp_data.TwoFactor2*tmp_data.TwoFactor1*0.0\n",
    "        # Add linear relationship to lp\n",
    "        lp = lp + tmp_data[lin_cols].dot(lin_coeff)\n",
    "\n",
    "    if bin_var_p > 0:\n",
    "        lp = lp + bin_coef * tmp_data['Binary1']\n",
    "        tmp_data['Binary1_prime'] = 1 - tmp_data['Binary1']\n",
    "\n",
    "    if outcome == 'classification':\n",
    "\n",
    "        # convert to a probability\n",
    "        prob = 1 / (1 + np.exp(-lp))\n",
    "\n",
    "        # add mislabelling if desired - TO DO: need to fix\n",
    "        if (mislabel > 0) and (mislabel < 1):\n",
    "            shuffle_rows = np.random.choice(n, np.floor(n * mislabel), replace=False)\n",
    "            prob[shuffle_rows] = 1 - prob[shuffle_rows]\n",
    "\n",
    "        # generate target using a random threshold based on a gaussian distrib between 0 and 1\n",
    "        tmp_data['target'] = np.where(prob <= np.random.uniform(size=n), 0, 1)\n",
    "\n",
    "    elif outcome == 'regression':\n",
    "\n",
    "        # continuous outcome based on linear predictor\n",
    "        tmp_data['target'] = np.random.normal(lp, regression_err, size=n)\n",
    "\n",
    "    return tmp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_var(df: pd.DataFrame, \n",
    "              feature_name: str, \n",
    "              min_: Union[int, float]=0, \n",
    "              max_: Union[int, float]=1) -> np.array: \n",
    "    \"\"\"\n",
    "    Takes in a data frame and applies a min-max scaler to given bounds for a single column\n",
    "    \"\"\"\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(min_, max_))\n",
    "    scaled_arr = scaler.fit_transform(df[[feature_name]]).reshape(1, -1)[0]\n",
    "    \n",
    "    return scaled_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refactor_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df.rename({ \n",
    "        \"TwoFactor1\": \"Weight on bit (kg)\", # higher weight --> higher weight will increase risks of danger \n",
    "        \"TwoFactor2\": \"Rotation speed (rpm)\", # Rotation speed of the drilling bit (too fast rotation can lead to overheating, too low rotation renders drilling mnore difficult)         \n",
    "        \"Linear1\": \"Depth of operation (m)\", # lower point of the well\n",
    "        \"Linear1_prime\": \"Hole diameter (m)\", # Diameter of the hole (diameter diminishes as depth increases)\n",
    "        \"Nonlinear1\": \"Mud Flow in (m3/s)\", # Speed of mud circulation\n",
    "        \"Linear2\": \"Mud density (kg/L)\", # need to have equal mud and soil density to avoid well collapse (formation falling in well and blocking pipe) or mud loss (mud flowing in the formation)\n",
    "        \"Linear3\": \"Rate of Penetration (ft/h)\", # higher RoP will provide less time for drilling engineers to observe real time data and adjust drilling parameter set up -> leading to a higher risk of incident (but more economic to drill faster)\n",
    "        \"Noise1\": \"Temperature (C)\", # Temperature at the drilling bit \n",
    "        \"target\": \"Incident\"\n",
    "    }, axis=1, inplace=True)\n",
    "    \n",
    "    scaling_dict = { \n",
    "        'Weight on bit (kg)': [100, 500], \n",
    "        'Rotation speed (rpm)': [900, 15000],\n",
    "        'Rate of Penetration (ft/h)': [10, 40],\n",
    "        #'Vertical depth of operation (m)': [0, 1500], \n",
    "        'Mud density (kg/L)': [0.5, 4],\n",
    "        'Hole diameter (m)': [0.5, 10], \n",
    "        'Temperature (C)': [0, 100], \n",
    "        'Depth of operation (m)': [0, 1500], \n",
    "        'Mud Flow in (m3/s)': [0, 100],\n",
    "        'Incident': [0, 1]\n",
    "    }\n",
    "\n",
    "    for k,v in scaling_dict.items(): \n",
    "        df.loc[:, k] = scale_var(df, k, v[0], v[1])\n",
    "        \n",
    "    df[\"Inverse Rate of Penetration (h/ft)\"] = 1/df[\"Rate of Penetration (ft/h)\"]\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```\n",
    "# simulate data and save for use in example\n",
    "\n",
    "df = data_sim(n=500,\n",
    "             intercept=0,\n",
    "             linear_vars=4,\n",
    "             noise_vars=1,\n",
    "             corr_vars=0,\n",
    "             corr_type=\"AR1\",\n",
    "             corr_value=0.4,\n",
    "             mislabel=0,\n",
    "             surg_err=0.05,\n",
    "             bin_var_p=0,\n",
    "             bin_coef=0,\n",
    "             outcome=\"classification\",\n",
    "             regression_err=0.2,\n",
    "             drilling_ex=True\n",
    "             )\n",
    "\n",
    "df = refactor_dataset(df)\n",
    "\n",
    "df.to_csv(\"water_drill_data_classification.csv\", sep=\";\", encoding=\"utf-8\", index=False)\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facet-develop",
   "language": "python",
   "name": "facet-develop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
