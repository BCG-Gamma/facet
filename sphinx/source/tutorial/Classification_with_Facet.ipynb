{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<img src=\"../_static/Gamma_Facet_Logo_RGB_LB.svg\" width=\"500\" style=\"padding-bottom: 70px; padding-top: 70px; margin: auto; display: block\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with FACET\n",
    "\n",
    "***\n",
    "\n",
    "**Robust and impactful Data Science with FACET**\n",
    "\n",
    "FACET enables us to perform several critical steps in best practice Data Science work flow easily, efficiently and reproducibly:\n",
    "\n",
    "1. Create a robust pipeline for learner selection using LearnerRanker and cross-validation.\n",
    "\n",
    "2. Enhance our model inspection to understand drivers of predictions using local explanations of features via [SHAP values](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions) by applying a novel methodology that decomposes SHAP values into measures of synergy, redundancy, and independence between each pair of features.\n",
    "\n",
    "3. Quickly apply historical simulation to gain key insights into feature values that minimize or maximize the predicted outcome.\n",
    "\n",
    "***\n",
    "\n",
    "**Context**\n",
    "\n",
    "Prediabetes is a treatable condition that leads to many health complications and eventually type 2 diabetes. Identification of  individuals at risk of prediabetes can improve early intervention and provide insights into those interventions that work best.\n",
    "Using a cohort of healthy (*n*=2847) and prediabetic (*n*=1509) patients derived \n",
    "from the [NHANES 2013-14 U.S. cross-sectional survey](https://wwwn.cdc.gov/nchs/nhanes/Search/DataPage.aspx?Component=Examination&CycleBeginYear=2013) we aim to create a classifier for prediabetes. For further details on data sources, definitions and the study cohort please see the Appendix ([Data source and study cohort](#Data-source-and-study-cohort)).\n",
    "\n",
    "Utilizing FACET, we will do the following:\n",
    "\n",
    "1. create a pipeline to find identify a well-performing classifier.\n",
    "2. perform model inspection and simulation to gain understanding and insight into key factors predictive of prediabetes.\n",
    "\n",
    "***\n",
    "\n",
    "**Tutorial outline**\n",
    "\n",
    "1. [Required imports](#Required-imports)\n",
    "2. [Preprocessing and initial feature selection](#Preprocessing-and-initial-feature-selection)\n",
    "3. [Selecting a learner using FACET ranker](#Selecting-a-learner-using-FACET-ranker)\n",
    "4. [Using the FACET inspector for model inspection](#Using-the-FACET-inspector-for-model-inspection)\n",
    "5. [FACET univariate simulator: the impact of waist to height ratio](#FACET-univariate-simulator:-the-impact-of-waist-to-height-ratio)\n",
    "6. [Summary](#Summary)\n",
    "7. [What can you do next?](#What-can-you-do-next?)\n",
    "8. [Appendix](#Appendix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "delete_for_interactive": true,
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# this cell's metadata contains\n",
    "# \"nbsphinx\": \"hidden\" so it is hidden by nbsphinx\n",
    "\n",
    "\n",
    "def _set_paths() -> None:\n",
    "    # set the correct path when launched from within PyCharm\n",
    "\n",
    "    module_paths = [\"pytools\", \"facet\", \"sklearndf\"]\n",
    "\n",
    "    import sys\n",
    "    import os\n",
    "\n",
    "    if \"cwd\" not in globals():\n",
    "        # noinspection PyGlobalUndefined\n",
    "        global cwd\n",
    "        cwd = os.path.join(os.getcwd(), os.pardir, os.pardir, os.pardir)\n",
    "        os.chdir(cwd)\n",
    "    print(f\"working dir is '{os.getcwd()}'\")\n",
    "    for module_path in module_paths:\n",
    "        if module_path not in sys.path:\n",
    "            sys.path.insert(0, os.path.abspath(f\"{cwd}/{os.pardir}/{module_path}/src\"))\n",
    "        print(f\"added `{sys.path[0]}` to python paths\")\n",
    "\n",
    "\n",
    "def _ignore_warnings():\n",
    "    # ignore irrelevant warnings that would affect the output of this tutorial notebook\n",
    "\n",
    "    # ignore a useless LGBM warning\n",
    "    import warnings\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, message=r\".*Xcode_8\\.3\\.3\")\n",
    "\n",
    "\n",
    "_set_paths()\n",
    "_ignore_warnings()\n",
    "\n",
    "del _set_paths, _ignore_warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell's metadata contains\n",
    "# \"nbsphinx\": \"hidden\" so it is hidden by nbsphinx\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def _configure_matplotlib():\n",
    "    # set global options for matplotlib\n",
    "\n",
    "    import matplotlib\n",
    "\n",
    "    matplotlib.rcParams[\"figure.figsize\"] = (16.0, 8.0)\n",
    "    matplotlib.rcParams[\"figure.dpi\"] = 72\n",
    "\n",
    "\n",
    "_configure_matplotlib()\n",
    "\n",
    "del _configure_matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run this notebook, we will import not only the FACET package, but also other packages useful to solve this task. Overall, we can break down the imports into three categories: \n",
    "\n",
    "1. Common packages (pandas, matplotlib, etc.)\n",
    "2. Required FACET classes (inspection, selection, validation, simulation, etc.)\n",
    "3. Other BCG Gamma packages which simplify pipelining (sklearndf, see on [GitHub](https://github.com/orgs/BCG-Gamma/sklearndf/)) and support visualization (pytools, see on [GitHub](https://github.com/BCG-Gamma/pytools)) when using FACET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common package imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import seaborn as sns\n",
    "from tableone import TableOne\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.model_selection import RepeatedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gamma FACET imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facet.data import Sample\n",
    "from facet.inspection import LearnerInspector\n",
    "from facet.selection import LearnerRanker, LearnerGrid\n",
    "from facet.validation import BootstrapCV\n",
    "from facet.simulation.partition import ContinuousRangePartitioner\n",
    "from facet.simulation import UnivariateProbabilitySimulator\n",
    "from facet.simulation.viz import SimulationDrawer\n",
    "from facet.crossfit import LearnerCrossfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = BootstrapCV(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.get_n_splits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sklearndf imports**\n",
    "\n",
    "Instead of using the \"regular\" scikit-learn package, we are going to use sklearndf (see on [GitHub](https://github.com/orgs/BCG-Gamma/sklearndf/)). sklearndf is an open source library designed to address a common issue with scikit-learn: the outputs of transformers are numpy arrays, even when the input is a data frame. However, to inspect a model it is essential to keep track of the feature names. sklearndf retains all the functionality available through scikit-learn plus the feature traceability and usability associated with Pandas data frames. Additionally, the names of all your favourite scikit-learn functions are the same except for `DF` on the end. For example, the standard scikit-learn import:\n",
    "\n",
    "`from sklearn.pipeline import Pipeline`\n",
    "\n",
    "becomes:\n",
    "\n",
    "`from sklearndf.pipeline import PipelineDF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearndf.pipeline import PipelineDF, ClassifierPipelineDF\n",
    "from sklearndf.classification import RandomForestClassifierDF\n",
    "from sklearndf.classification.extra import LGBMClassifierDF\n",
    "from sklearndf.transformation import (\n",
    "    ColumnTransformerDF,\n",
    "    OneHotEncoderDF,\n",
    "    SimpleImputerDF,\n",
    ")\n",
    "from sklearndf.transformation.extra import BorutaDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pytools imports**\n",
    "\n",
    "pytools (see on [GitHub](https://github.com/BCG-Gamma/pytools)) is an open source library containing general machine learning and visualization utilities, some of which are useful for visualising the advanced model inspection capabilities of FACET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytools.viz.dendrogram import DendrogramDrawer, LinkageTree\n",
    "from pytools.viz.matrix import MatrixDrawer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and initial feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to load our prediabetes data and create a simple preprocessing pipeline. For those interested some initial EDA can be found in the Appendix ([Exploratory Data Analysis](#Exploratory-Data-Analysis-(EDA)))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the prepared data frame\n",
    "prediab_df = pd.read_csv(\"sphinx/source/tutorial/pre_diab_nhanes.csv\")\n",
    "\n",
    "# create a couple of new interesting features\n",
    "prediab_df[\"SBP_to_DBP\"] = prediab_df[\"Average_SBP\"] / prediab_df[\"Average_DBP\"]\n",
    "prediab_df[\"Waist_to_hgt\"] = (\n",
    "    prediab_df[\"Waist_Circumference\"] / prediab_df[\"Standing_Height\"]\n",
    ")\n",
    "\n",
    "# make clear based on dtypes these two features are categorical\n",
    "prediab_df[\"General_health\"] = prediab_df[\"General_health\"].astype(\"object\")\n",
    "prediab_df[\"Healthy_diet\"] = prediab_df[\"Healthy_diet\"].astype(\"object\")\n",
    "\n",
    "# have a look\n",
    "prediab_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to ensure a quick run we will use a random sample of 1000 observations\n",
    "prediab_df = prediab_df.sample(n=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/markdown"
   },
   "source": [
    "For easier data management we will create a sample object using FACET's `Sample` class, which allows us to: \n",
    "\n",
    "- Quickly access the target vs. features\n",
    "- Pass our data into sklearndf pipelines\n",
    "- Pass information to other FACET functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a FACET sample object\n",
    "prediab = Sample(\n",
    "    observations=prediab_df,\n",
    "    feature_names=prediab_df.drop(columns=[\"Pre_diab\"]).columns,\n",
    "    target_name=\"Pre_diab\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "raw_mimetype": "text/markdown"
   },
   "source": [
    "Next we create a minimum preprocessing pipeline which based on our initial EDA ([Exploratory Data Analysis](#Exploratory-Data-Analysis-(EDA))) needs to address the following:\n",
    "\n",
    "1. Simple imputation for missing values in both continuous and categorical features\n",
    "2. One-hot encoding for categorical features\n",
    "\n",
    "We will use the sklearndf wrappers for scikit-learn functions such as `SimpleImputerDF` in place of `SimpleImputer`, `OneHotEncoderDF` in place of `OneHotEncoder`, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for categorical features we will use the mode as the imputation value and also one-hot encode\n",
    "preprocessing_categorical = PipelineDF(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputerDF(strategy=\"most_frequent\", fill_value=\"<na>\")),\n",
    "        (\"one-hot\", OneHotEncoderDF(sparse=False, handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# for numeric features we will impute using the median\n",
    "preprocessing_numerical = SimpleImputerDF(strategy=\"median\")\n",
    "\n",
    "# put the pipeline together\n",
    "preprocessing_features = ColumnTransformerDF(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"categorical\",\n",
    "            preprocessing_categorical,\n",
    "            make_column_selector(dtype_include=object),\n",
    "        ),\n",
    "        (\n",
    "            \"numerical\",\n",
    "            preprocessing_numerical,\n",
    "            make_column_selector(dtype_include=np.number),\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we perform some initial feature selection using Boruta, a recent approach shown to have quite good performance. The Boruta algorithm removes features that are no more predictive than random noise. If you are interested further, please see this  [article](https://www.jstatsoft.org/article/view/v036i11).\n",
    "\n",
    "The `BorutaDF` transformer in our sklearndf package provides easy access to this powerful method. The approach relies on a tree-based learner, usually a random forest. For settings, a `max_depth` of between 3 and 7 is typically recommended, and here we rely on the default setting of 5. However, as this depends on the number of features and the complexity of interactions one could also explore the sensitivity of feature selection to this parameter. The number of trees is automatically managed by the Boruta feature selector argument `n_estimators=\"auto\"`.\n",
    "\n",
    "We also use parallelization for the random forest using `n_jobs` to accelerate the Boruta iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create the pipeline for Boruta\n",
    "boruta_feature_selection = PipelineDF(\n",
    "    steps=[\n",
    "        (\"preprocessing\", preprocessing_features),\n",
    "        (\n",
    "            \"boruta\",\n",
    "            BorutaDF(\n",
    "                estimator=RandomForestClassifierDF(\n",
    "                    max_depth=5, n_jobs=-3, random_state=42\n",
    "                ),\n",
    "                n_estimators=\"auto\",\n",
    "                random_state=42,\n",
    "                verbose=False,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# run feature selection using Boruta and report those selected\n",
    "boruta_feature_selection.fit(X=prediab.features, y=prediab.target)\n",
    "selected = boruta_feature_selection.feature_names_original_.unique()\n",
    "selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boruta identified 10 features (out of a potential 47) that we will retain in our FACET sample object for classification. Note that this feature selection process could be included in a general preprocessing pipeline, however due to the computation involved, we have utilized Boruta here as an initial one-off processing step to narrow down the features for our classifier development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# update FACET sample object to only those features Boruta identified as useful\n",
    "prediab_initial_features = prediab.keep(feature_names=selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting a learner using FACET ranker\n",
    "\n",
    "FACET implements several additional useful wrappers which further simplify comparing and tuning a larger number of models and configurations: \n",
    "\n",
    "- `LearnerGrid`: allows you to pass a learner pipeline (i.e., classifier + any preprocessing) and a set of hyperparameters\n",
    "- `LearnerRanker`: multiple LearnerGrids can be passed into this class as a list - this allows tuning hyperparameters both across different types of learners in a single step and ranks the resulting models accordingly\n",
    "\n",
    "The following learners and hyperparameter ranges will be assessed using 10 repeated 5-fold cross-validation:\n",
    "\n",
    "\n",
    "1. **Random forest**: with hyperparameters\n",
    "    - max_leaf_nodes: [5, 10, 20]\n",
    "    - n_estimators: [50, 100, 200]  \n",
    "  \n",
    "  \n",
    "2. **Light gradient boosting**: with hyperparameters\n",
    "    - max_depth: [5, 10]\n",
    "    - num_leaves: [20]\n",
    "    - learning_rate: [0.2, 0.1]\n",
    "    - subsample: [0.7]\n",
    "    - feature_fraction: [0.8]\n",
    "\n",
    "Note if you want to see a list of hyperparameters you can use `classifier_name().get_params().keys()` where `classifier_name` could be for example `RandomForestClassifierDF` and if you want to see the default values, just use `classifier_name().get_params()`.\n",
    "\n",
    "Finally, for this exercise we will use AUC as the performance metric for scoring and ranking our classifiers (the default is accuracy). Note that ranking uses the average performance minus two times the standard deviation, so that we consider both the average performance and variability when selecting a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we specify the classifiers we want to train using `ClassifierPipelineDF` from sklearndf. Note here we also include the feature preprocessing steps we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest learner\n",
    "rforest_clf = ClassifierPipelineDF(\n",
    "    preprocessing=preprocessing_features,\n",
    "    classifier=RandomForestClassifierDF(random_state=42),\n",
    ")\n",
    "\n",
    "# light gradient boosting learner\n",
    "lgbm_clf = ClassifierPipelineDF(\n",
    "    preprocessing=preprocessing_features,\n",
    "    classifier=LGBMClassifierDF(random_state=42)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a list of learner grids where each learner grid is created using `LearnerGrid` and allows us to associate a `ClassifierPipelineDF` with a specified set of hyperparameter via the `learner_parameters` argument. Note this structure allows us to easily include additional classifiers and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_grid = [\n",
    "    LearnerGrid(\n",
    "        pipeline=rforest_clf,\n",
    "        learner_parameters={\"max_depth\": [4, 8, 16, 32], \"n_estimators\": [200, 500]},\n",
    "    ),\n",
    "    LearnerGrid(\n",
    "        pipeline=lgbm_clf,\n",
    "        learner_parameters={\n",
    "            \"max_depth\": [5, 10],\n",
    "            \"num_leaves\": [20],\n",
    "            \"learning_rate\": [0.2, 0.1],\n",
    "            \"subsample\": [0.7],\n",
    "            \"feature_fraction\": [0.8],\n",
    "        },\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit the grid defined above using the `LeanerRanker`, which will run a gridsearch (or random search if defined) using 10 repeated 5-fold cross-validation on our selected set of features from Boruta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ranker = LearnerRanker(\n",
    "    grids=classifier_grid,\n",
    "    cv=RepeatedKFold(n_splits=5, n_repeats=10, random_state=42),\n",
    "    n_jobs=-3,\n",
    "    scoring=\"roc_auc\",\n",
    ").fit(prediab_initial_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how each model scored using the `summary_report()` method of the `LearnerRanker`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at performance for the top 5 ranked classifiers\n",
    "print(clf_ranker.summary_report(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see based on our learner ranker, we have selected a Random Forest algorithm that achieved a mean ROC AUC of 0.72 with a SD of 0.03."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the FACET inspector for model inspection\n",
    "\n",
    "The [SHAP approach](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions) has become the standard method for model inspection. SHAP values are used to explain the additive contribution of each feature to the prediction for a given observation. SHAP values are computed for every feature and observation.\n",
    "\n",
    "The FACET `LearnerInspector` computes SHAP values for each crossfit (i.e., a CV fold or bootstrap resample) using the best model identified by the `LearnerRanker`. The FACET `LearnerInspector` then provides advanced model inspection through new SHAP-based summary metrics for understanding feature redundancy and synergy. Redundancy and synergy are calculated using the new algorithm FACET implements for using SHAP values to understand model predictions.\n",
    "\n",
    "The definitions are as follows:\n",
    "\n",
    "- **Redundancy** represents how much information is shared between two features contributions to the model predictions. For example, given features X and Y as coordinates on a chess board, the colour of a square can only be predicted when considering X and Y in combination. Redundancy is expressed as a percentage ranging from 0% (full uniqueness) to 100% (full redundancy).\n",
    "\n",
    "\n",
    "- **Synergy** represents how much the combined information of two features contributes to the model predictions. For example, temperature and pressure in a pressure cooker are redundant features for predicting cooking time since pressure will rise relative to the temperature, and vice versa. Therefore, knowing just one of either temperature or pressure will likely enable the same predictive accuracy. Synergy is expressed as a percentage ranging from 0% (full autonomy) to 100% (full synergy).\n",
    "\n",
    "Both cases can apply at the same time, i.e. a pair of features can use some information synergistically while using other information redundantly.\n",
    "\n",
    "To analyse redundancy for all possible feature parings, the approach is:\n",
    "\n",
    "1.\tCalculate the feature redundancy matrix using SHAP value decomposition - this gives us pairwise redundancy between features, in the range of 0.0 (fully unique contributions) and 1.0 (fully redundant contributions)\n",
    "2.\tTransform the feature redundancy matrix into a feature distance matrix, where distance is expressed as (1.0 - redundancy)\n",
    "3.\tPerform hierarchical, single-linkage clustering on the distance matrix, thus identifying groups of low-distance, redundant features which activate “in tandem” to predict the outcome\n",
    "\n",
    "The same approach can be used to analyse synergy.\n",
    "\n",
    "The inspector can calculate all of this with a single method call, but also offers methods to access the intermediate results of each step. A lightweight visualization framework is available to render the results in different styles.\n",
    "\n",
    "SHAP values from the `LearnerInspector` can also be used with the SHAP package plotting functions for sample and observation level SHAP visualizations, such as SHAP distribution plots, dependency plots, force plots and waterfall plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run inspector\n",
    "clf_inspector = LearnerInspector(\n",
    "    n_jobs=-3,\n",
    "    verbose=False,\n",
    ").fit(crossfit=clf_ranker.best_model_crossfit_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain FACET feature importance, as well as synergy and redundancy matrices\n",
    "f_importance = clf_inspector.feature_importance()\n",
    "redundancy_matrix = clf_inspector.feature_redundancy_matrix()\n",
    "synergy_matrix = clf_inspector.feature_synergy_matrix()\n",
    "dd_redundancy = clf_inspector.feature_redundancy_linkage()\n",
    "\n",
    "# also let's get some info for standard SHAP plots\n",
    "shap_values = clf_inspector.shap_values().to_numpy()\n",
    "X_train = rforest_clf.preprocessing.fit_transform(X=prediab_initial_features.features)\n",
    "X_train = X_train.reset_index()[prediab_initial_features.features.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance\n",
    "\n",
    "Feature importance has many ways of being measured. Here we utilize the FACET implementation based on the `LearnerInspector`. Each feature is ranked according to the mean SHAP value for that feature. This plot is paired with a standard SHAP distribution plot for features to see if there is any directional tendency for the associations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FACET feature importance\n",
    "plt.subplot(1, 2, 1)\n",
    "f_importance.sort_values().plot.barh()\n",
    "\n",
    "# standard SHAP summary plot using the shap package\n",
    "plt.subplot(1, 2, 2)\n",
    "shap.summary_plot(shap_values, X_train, show=False, plot_size=(16.0, 8.0))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the feature importance's we can see the top five features are age, waist to height ratio, RBC count, average systolic blood pressure and waist circumference. Inspection of the SHAP value distributions does not provide any indication of a general direction of association for any features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synergy and redundancy\n",
    "\n",
    "Synergy and redundancy are part of the key extensions FACET makes to using SHAP values to understand model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synergy heatmap\n",
    "MatrixDrawer(style=\"matplot%\").draw(synergy_matrix, title=\"Feature synergies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Synergy represents the degree to which one feature combines with another to generate a prediction.* In the above heatmap we can see the off-diagonal values never exceed 15%. This would suggest that no two features have a strong degree of synergy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redundancy heatmap\n",
    "MatrixDrawer(style=\"matplot%\").draw(redundancy_matrix, title=\"Feature redundancies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Redundancy represents the shared information between two features.* In the above heatmap we can see a values of 49% and higher for (`Waist_circumference` and `Waist_to_hgt`), (`Waist_circumference` and `BMI`) and (`BMI` and `Waist_to_hgt`). Another way to look at redundancy is using a dendrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redundancy dendrogram\n",
    "DendrogramDrawer().draw(title=\"Redundancy linkage\", data=dd_redundancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dendrogram represents the extent of clustering among the features.\n",
    "\n",
    "To obtain the hierarchical clustering, we calculate a linkage tree and plug it into a dendrogram drawer. This makes it easy to visually single out features that are both important and mutually redundant.\n",
    "\n",
    "Taking the `Waist_circumference` and `Waist_to_hgt` features which have the highest redundancy, we can see these features cluster together earliest in the dendrogram. Ideally, we want to see features only start to cluster as close to the right-hand side of the dendrogram as possible. This implies all features in the model are contributing uniquely to our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What might we infer from the above information?**\n",
    "\n",
    "1. BMI, waist circumference and waist to height ratio form a small cluster of redundant features. This seems reasonable:\n",
    "    * waist circumference is included in the calculation of waist to height ratio\n",
    "    * we might expect BMI to capture similar information about excess body mass as higher waist circumference and waist to height ratio\n",
    "2. We saw little synergy between features. We might have expected apriori to find some interesting synergies between diet, exercise, sleep and body composition. Of course, the model needs to identify these relationships from them to be reflected in the synergy metric(s).\n",
    "\n",
    "**What action(s) might we take?**\n",
    "\n",
    "1. Given the redundancy that appears between BMI, waist circumference and waist to height ratio, we could look to eliminate one or two of these features from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience when working in a non-notebook environment, all of the `Drawer`s provided by the [pytools](https://github.com/BCG-Gamma/pytools) package also support a `style='text'` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DendrogramDrawer(style=\"text\").draw(title=\"Redundancy linkage\", data=dd_redundancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing redundant features\n",
    "\n",
    "Recall the redundancy dendrogram above where we saw a clear cluster of features with redundancy; `Waist_to_hgt`, `BMI`, and `Waist_Circumference`.\n",
    "\n",
    "- assess if the features of the model are unique, i.e. not redundant with other features\n",
    "- decide which features to discard, combine, or modify to increase the uniqueness of important features in the model\n",
    "\n",
    "Before we proceed to looking at SHAP values for individual predictions and perform a univariate simulation, let's eliminate two partially redundant features - we will choose to keep `Waist_to_hgt` ratio and drop `BMI` and `Waist_Circumference`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop redundant features from our FACET sample object\n",
    "prediab_no_redundant_feat = prediab_initial_features.drop(\n",
    "    [\"BMI\", \"Waist_Circumference\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# re-run ranker without redundant features\n",
    "clf_ranker = LearnerRanker(\n",
    "    grids=classifier_grid,\n",
    "    cv=RepeatedKFold(n_splits=5, n_repeats=10, random_state=42),\n",
    "    n_jobs=-3,\n",
    "    scoring=\"roc_auc\",\n",
    ").fit(prediab_no_redundant_feat)\n",
    "\n",
    "# run inspector\n",
    "inspector_no_redun = LearnerInspector(\n",
    "    n_jobs=-3,\n",
    "    verbose=False,\n",
    ").fit(crossfit=clf_ranker.best_model_crossfit_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redundancy dendrogram\n",
    "dd_redundancy = inspector_no_redun.feature_redundancy_linkage()\n",
    "DendrogramDrawer().draw(title=\"HD set features\", data=dd_redundancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with the removal of `BMI` and `Waist_Circumference` we can see the feature clustering starts much further to the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the best ranked model after removing redundant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ranker.best_model_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FACET univariate simulator: the impact of waist to height ratio\n",
    "\n",
    "Another advantage of FACET is the ability to quickly instigate and run univariate simulation.\n",
    "Simulation enables us to gain insight into what value(s) of this ratio might minimize the likelihood of prediabetes.\n",
    "\n",
    "As the basis for the simulation, we divide the feature into relevant partitions: \n",
    "\n",
    "- We use FACET's `ContinuousRangePartitioner` to split the range of observed values of waist to height ratio into intervals of equal size. Each partition is represented by the central value of that partition. \n",
    "- For each partition, the simulator creates an artificial copy of the original sample assuming the variable to be simulated has the same value across all observations - which is the value representing the partition. Using the best `LearnerCrossfit` acquired from the ranker, the simulator now re-predicts all targets using the models trained for all folds and determines the average value of the target variable resulting from this.\n",
    "- The FACET `SimulationDrawer` allows us to visualise the result; both in a matplotlib and a plain-text style\n",
    "\n",
    "Finally, because FACET can use bootstrap cross validation, we can create a crossfit from our previous `LearnerRanker` best model to perform the simulation so we can quantify the uncertainty by using bootstrap confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a bootstrap CV crossfit for simulation using best model\n",
    "boot_crossfit = LearnerCrossfit(\n",
    "    pipeline=clf_ranker.best_model_.native_estimator,\n",
    "    cv=BootstrapCV(n_splits=250, random_state=42),\n",
    "    n_jobs=-3,\n",
    "    verbose=2,\n",
    ").fit(sample=prediab_no_redundant_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up and run a simulation\n",
    "sim_feature = \"Waist_to_hgt\"\n",
    "waist_to_hgt_simulator = UnivariateProbabilitySimulator(\n",
    "    crossfit=boot_crossfit, n_jobs=-1\n",
    ")\n",
    "waist_to_hgt_partitions = ContinuousRangePartitioner()\n",
    "waist_to_hgt_simulation = waist_to_hgt_simulator.simulate_feature(\n",
    "    name=sim_feature, partitioner=waist_to_hgt_partitions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the results\n",
    "SimulationDrawer().draw(data=waist_to_hgt_simulation, title=sim_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the simulation shows that higher waist to height ratios are associated with an increased risk of prediabetes. We could also suggest that keeping a person's waist to height ratio below 0.52 will reduce the likelihood of prediabetes from around 34% to 30%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waist_to_hgt_simulation = waist_to_hgt_simulator.simulate_actuals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(waist_to_hgt_simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can also get a print out of simulation results\n",
    "SimulationDrawer(\"text\").draw(data=waist_to_hgt_simulation, title=sim_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "With the capabilities offered by FACET we were able to:\n",
    "\n",
    "1. Identify a learner with performance comparable to models in the literature.\n",
    "2. Utilize advanced the SHAP value capabilities (synergy and redundancy) to identify additional features that could be removed (i.e., BMI and waist circumference removed in favour of waist to height ratio) and whether any features had strong synergistic effects - which they did not.\n",
    "3. Simulate the effect of changes in waist to height ratio on the likelihood of being prediabetic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What can you do next?\n",
    "\n",
    "There are several next/alternative steps that could be taken:\n",
    "\n",
    "1. Utilize methods to deal with class imbalance and see if it improves the model.\n",
    "2. Adding more features! The NHANES data is a treasure trove of information.\n",
    "3. Retain diabetic patients and convert it into a multi-class learning problem.\n",
    "4. What would happen if we applied survey weights when constructing a learner?\n",
    "5. Further investigation of feature engineering. One could also look at different sets of measurements such as the bio-profile and perform dimension reduction first via PCA or some other method.\n",
    "6. Other learners such as SVC, LDA, Elastic-Net, CNN.\n",
    "7. More sophisticated imputation for missing values: the assumption of MAR might not hold, as those with worse health and thus more at risk of prediabetes may be more likely not to disclose poor health characteristics. Methods enabled by IterativeImputer could be used or even KNNImputer. Also feature engineering could be done post imputation in the pipeline, so values such as ratios are consistent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data source and study cohort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction**  \n",
    "Prediabetes is a treatable condition that leads to many health complications, including eventually type 2 diabetes. Prediabetes has become an epidemic worldwide and is increasing in prevalence. As a largely asymptomatic condition, screening for prediabetes can be extremely challenging. However, early intervention, especially with lifestyle changes has been shown as effective in treating prediabetes. Accurate prediction/identification of those individuals at risk of prediabetes can improve early intervention and may provide insights into those interventions that work best. The current standard of care is a CDC prediabetes risk [screening tool](https://www.cdc.gov/diabetes/prevention/pdf/Prediabetes-Risk-Test-Final.pdf). \n",
    "\n",
    "**Data source**  \n",
    "The dataset used in this tutorial is derived from the [National Health and Nutrition Examination Survey (NHANES) 2013-14 cross-sectional survey](https://wwwn.cdc.gov/nchs/nhanes/Search/DataPage.aspx?Component=Examination&CycleBeginYear=2013). In brief, NHANES collects demographic, socioeconomic, dietary, health, medical, dental, physiological and laboratory data on a nationally representative sample of noninstitutionalized, civilian United States residents. Please note the set-up for this data loosely follows the approach in [De Silva et al](https://pubmed.ncbi.nlm.nih.gov/31889178/).\n",
    "\n",
    "**Patient cohort**  \n",
    "In the NHANES data sub-sets of those surveyed may undergo a plasma glucose (FPG) test, oral glucose tolerance\n",
    "test (OGTT), or have glycated haemoglobin (HbA1c) measured. Diabetic patients were defined as those with any of the following: FPG >= 126 mg/dl, OGTT > 200 mg/dl, HbA1c > 6.4% or a Doctor diagnosed diabetes. The created dataset contains selected information for 4356 patients aged 20 years or over who were not considered diabetic or who were not pregnant or suspected to be pregnant at the time of the survey.\n",
    "\n",
    "**Learning target: prediabetes status**  \n",
    "Using any of the available FPG, OGTT and HbA1c tests we defined patients as prediabetic where any of the following was satisfied: FPG 100–125 mg/dl, OGTT 140–200 mg/dl, or HbA1c 5.7–6.4%. Among this cohort 35% were prediabetic (n=1509).\n",
    "\n",
    "**Initial features**  \n",
    "The following tables provides an overview of the 37 features included in the example dataset.\n",
    "\n",
    "|Instrument\t|Data File Name (File)\t| NHANES Field\t| Description | Dataset name | Type |\n",
    "| :-- | :-- | :-- | :-- | :-- | :-- |\n",
    "|Demographics|Demographic Variables, Sample Weights (DEMO_H)|RIDAGEYR|Age in years at screening|Age|Numeric|\n",
    "|Demographics|Demographic Variables, Sample Weights (DEMO_H)|RIAGENDR|Gender|Gender| Categorical|\n",
    "|Examination|Body Measures (BMX_H)|BMXWT|Weight (kg)|Weight|Numeric|\n",
    "|Examination|Body Measures (BMX_H)|BMXHT|Standing Height (cm)|Standing_Height|Numeric|\n",
    "|Examination|Body Measures (BMX_H)|BMXWAIST|Waist Circumference (cm)|Waist_Circumference|Numeric|\n",
    "|Examination|Body Measures (BMX_H)|BMXBMI|Body Mass Index (kg/m^2)|BMI|Numeric|\n",
    "|Examination|Blood Pressure (BPX_H)|BPXSY1 to 4|Systolic: Blood pres mm Hg|Average_SBP| Numeric|\n",
    "|Examination|Blood Pressure (BPX_H)|BPXDI1 to 4|Diastolic: Blood pres mm Hg|Average_DBP| Numeric|\n",
    "|Questionnaire|Blood Pressure & Cholesterol (BPQ_H)|BPQ020|Ever told you had high blood pressure|High_BP| Categorical|\n",
    "|Questionnaire|Diet Behavior & Nutrition (DBQ_H)|DBQ700|How healthy is the diet|Healthy_diet| Categorical|\n",
    "|Questionnaire|Diabetes (DIQ_H)|DIQ175A|Family history|Family_hist_diab| Categorical|\n",
    "|Questionnaire|Diabetes (DIQ_H)|DIQ172|Feel could be at risk for diabetes|Feel_at_risk_diab| Categorical|\n",
    "|Questionnaire|Current Health Status (HSQ_H)|HSD010|General health condition|General_health| Categorical|\n",
    "|Questionnaire|Medical Conditions (MCQ_H)|MCQ080|Doctor ever said you were overweight|Told_overweight| Categorical|\n",
    "|Questionnaire|Physical Activity (PAQ_H)|PAQ605|Vigorous work activity|Vigorous_work_activity| Categorical|\n",
    "|Questionnaire|Physical Activity (PAQ_H)|PAQ620|Moderate work activity|Moderate_work_activity| Categorical|\n",
    "|Questionnaire|Physical Activity (PAQ_H)|PAQ635|Walk or bicycle|Walk_or_bicycle| Categorical|\n",
    "|Questionnaire|Physical Activity (PAQ_H)|PAQ650|Vigorous recreational activities|Vigorous_rec_activity| Categorical|\n",
    "|Questionnaire|Physical Activity (PAQ_H)|PAQ665|Moderate recreational activities|Moderate_rec_activity| Categorical|\n",
    "|Questionnaire|Sleep Disorders (SLQ_H)|SLD010H|How much sleep do you get (hours)?|Sleep_hours| Numeric|\n",
    "|Questionnaire|Sleep Disorders (SLQ_H)|SLQ050|Ever told doctor had trouble sleeping?|Trouble_sleeping| Categorical|\n",
    "|Questionnaire|Sleep Disorders (SLQ_H)|SLQ060|Ever told by doctor have sleep disorder?|Sleep_disorder| Categorical|\n",
    "|Questionnaire|Weight History (WHQ_H)|WHQ070|Tried to lose weight in past year|Tried_weight_loss_past_year| Categorical|\n",
    "|Laboratory|Cholesterol HDL (HDL_H)|LBDHDD|Direct HDL-Cholesterol (mg/dL)|HDL_Cholesterol| Numeric|\n",
    "|Laboratory|Cholesterol Total (TCHOL_H)|LBXTC|Total Cholesterol(mg/dL)|Total_Cholesterol| Numeric|\n",
    "|Laboratory|Complete Blood Count (CBC_H)|LBXWBCSI|White blood cell count (1000 cells/uL)|WBC_count| Numeric|\n",
    "|Laboratory|Complete Blood Count (CBC_H)|LBXRBCSI|Red blood cell count (million cells/uL)|RBC_count| Numeric|\n",
    "|Laboratory|Complete Blood Count (CBC_H)|LBXHCT|Haematocrit (%)|Haematocrit| Numeric|\n",
    "|Laboratory|Biochemistry Profile (BIOPRO_H)|LBXSTR|Triglycerides (mg/dL)|Triglycerides| Numeric|\n",
    "|Laboratory|Biochemistry Profile (BIOPRO_H)|LBXSUA|Uric acid (mg/dL)|Uric_acid| Numeric|\n",
    "|Laboratory|Biochemistry Profile (BIOPRO_H)|LBXSOSSI|Osmolality (mmol/Kg)|Osmolality| Numeric|\n",
    "|Laboratory|Biochemistry Profile (BIOPRO_H)|LBXSNASI|Sodium (mmol/L)|Sodium| Numeric|\n",
    "|Laboratory|Biochemistry Profile (BIOPRO_H)|LBXSKSI|Potassium (mmol/L)|Potassium| Numeric|\n",
    "|Laboratory|Biochemistry Profile (BIOPRO_H)|LBXSGTSI|Gamma glutamyl transferase (U/L)|Gamma_glutamyl_transferase| Numeric|\n",
    "|Laboratory|Biochemistry Profile (BIOPRO_H)|LBXSCA|Total calcium (mg/dL)|Calcium| Numeric|\n",
    "|Laboratory|Biochemistry Profile (BIOPRO_H)|LBXSATSI|Alanine aminotransferase ALT (IU/L)|Alanine_aminotransferase| Numeric|\n",
    "|Laboratory|Biochemistry Profile (BIOPRO_H)|LBXSASSI|Aspartate aminotransferase AST (IU/L)|Aspartate_aminotransferase| Numeric|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by doing some brief exploratory data analysis to assess the impact features might have on the likelihood someone is prediabetic and to also determine what will need to be addressed in a preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the prepared data frame\n",
    "prediab_eda = pd.read_csv(\"sphinx/source/tutorial/pre_diab_nhanes.csv\")\n",
    "prediab_eda.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also consider some rudimentary feature engineering as well, such as the ratio of waist circumference to height or the ratio of systolic to diastolic blood pressure. Let's create these two features as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediab_eda[\"SBP_to_DBP\"] = prediab_eda[\"Average_SBP\"] / prediab_eda[\"Average_DBP\"]\n",
    "prediab_eda[\"Waist_to_hgt\"] = (\n",
    "    prediab_eda[\"Waist_Circumference\"] / prediab_eda[\"Standing_Height\"]\n",
    ")\n",
    "prediab_eda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first a quick look at features overall\n",
    "prediab_eda.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missingness\n",
    "miss_count = prediab_eda.isna().sum()\n",
    "miss_pct = miss_count[miss_count > 0] / len(prediab_eda)\n",
    "miss_pct.sort_values().plot.barh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# those variables that are complete\n",
    "miss_count[miss_count == 0] / len(prediab_eda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view correlations with a heatmap\n",
    "df_cor = prediab_eda.corr()\n",
    "sns.heatmap(df_cor, xticklabels=df_cor.columns, yticklabels=df_cor.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# let's do a table comparing features by the target\n",
    "categorical = [\n",
    "    \"Gender\",\n",
    "    \"High_BP\",\n",
    "    \"Trouble_sleeping\",\n",
    "    \"Sleep_disorder\",\n",
    "    \"Told_overweight\",\n",
    "    \"General_health\",\n",
    "    \"Family_hist_diab\",\n",
    "    \"Feel_at_risk_diab\",\n",
    "    \"Vigorous_work_activity\",\n",
    "    \"Moderate_work_activity\",\n",
    "    \"Walk_or_bicycle\",\n",
    "    \"Vigorous_rec_activity\",\n",
    "    \"Moderate_rec_activity\",\n",
    "    \"Tried_weight_loss_past_year\",\n",
    "    \"Healthy_diet\",\n",
    "]\n",
    "\n",
    "mytable = TableOne(\n",
    "    prediab_eda,\n",
    "    columns=prediab_eda.columns.drop(\"Pre_diab\").to_list(),\n",
    "    categorical=categorical,\n",
    "    groupby=\"Pre_diab\",\n",
    "    pval=True,\n",
    "    remarks=False,\n",
    "    overall=False,\n",
    ")\n",
    "print(mytable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KDE plots by prediabetes status as well for those continuous features\n",
    "distn_vars = [\n",
    "    \"Age\",\n",
    "    \"Waist_Circumference\",\n",
    "    \"Weight\",\n",
    "    \"Standing_Height\",\n",
    "    \"BMI\",\n",
    "    \"Average_SBP\",\n",
    "    \"Average_DBP\",\n",
    "    \"HDL_Cholesterol\",\n",
    "    \"Total_Cholesterol\",\n",
    "    \"Sleep_hours\",\n",
    "    \"WBC_count\",\n",
    "    \"RBC_count\",\n",
    "    \"Hematocrit\",\n",
    "    \"Triglycerides\",\n",
    "    \"Uric_acid\",\n",
    "    \"Osmolality\",\n",
    "    \"Sodium\",\n",
    "    \"Potassium\",\n",
    "    \"Gamma_glutamyl_transferase\",\n",
    "    \"Calcium\",\n",
    "    \"Alanine_aminotransferase\",\n",
    "    \"Aspartate_aminotransferase\",\n",
    "    \"SBP_to_DBP\",\n",
    "    \"Waist_to_hgt\",\n",
    "]\n",
    "\n",
    "df_kde = pd.melt(prediab_eda[distn_vars + [\"Pre_diab\"]], \"Pre_diab\", distn_vars)\n",
    "g = sns.FacetGrid(\n",
    "    df_kde, col=\"variable\", hue=\"Pre_diab\", col_wrap=5, sharex=False, sharey=False\n",
    ")\n",
    "g.map(sns.kdeplot, \"value\", shade=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick EDA summary:**\n",
    "\n",
    "Missingness\n",
    "\n",
    "- Our target and 10 features were complete.\n",
    "- The other 29 features had levels of missing ranging from 0.02% to 11.5%, and 22 of the 29 were <3%.\n",
    "- Most extreme cases of missingness were for tried weight loss in the past year (11.5%) and general health (7.8%).\n",
    "\n",
    "Correlations\n",
    "\n",
    "- There is a wide range of correlation among features, where we can see that for example RBC count, haematocrit and standing height are all moderately negatively correlated with gender (i.e., females are shorter), and that body measurements for weight, height, BMI, and waist circumference are all strongly positively correlated. \n",
    "\n",
    "Associations\n",
    "\n",
    "- Some features already appear to not to be strongly associated based on univariate tests, such as: average DBP, trouble sleeping, vigorous work activity, moderate work activity, tried weight loss in the past year, healthy diet, WBC count, and calcium levels.\n",
    "- Features associated with an increased risk of prediabetes include: older age, being male, increased waist circumference, decreased standing height, increased weight, increased BMI, increased average SBP, lower HDL cholesterol, increased total cholesterol, having high BP, increased sleep hours, having a sleep disorder, being told you are overweight, poorer general health, family history of diabetes, feeling at risk for diabetes, reduced walking or cycling, less vigorous or moderate recreational activity, higher RBC count, higher haematocrit, increased triglycerides, increased uric acid, higher osmolality, increased sodium, increased potassium, increased gamma glutamyl transferase, increased alanine aminotransferase, increased aspartate aminotransferase, increased SBP to DBP ratio, increased waist to height ratio.\n",
    "\n",
    "Distributions of numeric features\n",
    "\n",
    "- Many of the continuous features have been flagged as potentially non-normal/multi-modal and with potential outliers. This can be seen in the plots where many biomarkers have positive skewed distributions and in the case of sodium, some modality which could also be related to measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facet-develop",
   "language": "python",
   "name": "facet-develop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
