{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<img src=\"../_static/Gamma_Facet_Logo_RGB_LB.svg\" width=\"500\" style=\"padding-bottom: 70px; padding-top: 70px; margin: auto; display: block\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression with FACET\n",
    "\n",
    "***\n",
    "\n",
    "**Robust and impactful Data Science with FACET**\n",
    "\n",
    "FACET enables us to perform several critical steps in best practice Data Science work flow easily, efficiently and reproducibly:\n",
    "\n",
    "1. Create a robust pipeline for learner selection using LearnerRanker and cross-validation.\n",
    "\n",
    "2. Enhance our model inspection to understand drivers of predictions using local explanations of features via SHAP values by applying a novel methodology that decomposes SHAP values into measures of synergy, redundancy, and independence between each pair of features.\n",
    "\n",
    "3. Quickly apply historical simulation to gain key insights into feature values that minimize or maximize the predicted outcome.\n",
    "\n",
    "***\n",
    "\n",
    "**Context**\n",
    "\n",
    "The data are from simulated experiments of a naval vessel (Frigate) Gas Turbine (GT) propulsion plant. The simulator considers the performance decay over time of the GT components such as GT compressor and turbines. One observation in this dataset represents the current decay states of the compressor and the gas turbine along with several sensor readings of the vessel at that point in time. The decay state is a performance metric [0, 1] where 1 means delivering 100% of nominal performance. We want to determine the machine settings which maximize the gas turbine decay state coefficient.\n",
    "\n",
    "Utilizing FACET, we will: \n",
    "\n",
    "1. Predict the decay state of the gas turbine as accurately as possible\n",
    "2. Understand which parameters drive the decay state of the turbine\n",
    "3. Analyse how these drivers interact with each other and the target\n",
    "\n",
    "While we can solve questions 1 and parts of question 2 with commonly used machine learning packages, FACET will enable us to make better inferences about the way some of the features share or complement information and help us to figure out the optimal settings of the vessel to minimize the equipment degradation at a variety of ship speeds.\n",
    "\n",
    "***\n",
    "\n",
    "**Tutorial outline**\n",
    "\n",
    "1. [Required imports](#Required-imports)\n",
    "2. [Preprocessing and initial feature selection](#Preprocessing-and-initial-feature-selection)\n",
    "3. [Selecting a learner using FACET ranker](#Selecting-a-learner-using-FACET-ranker)\n",
    "4. [Using the FACET inspector for model inspection](#Using-the-FACET-inspector-for-model-inspection)\n",
    "5. [FACET univariate simulator: the impact of waist to height ratio](#FACET-univariate-simulator:-the-impact-of-waist-to-height-ratio)\n",
    "6. [Appendix](#Appendix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-28T14:29:36.416582Z",
     "start_time": "2020-08-28T14:29:36.410164Z"
    },
    "delete_for_interactive": true,
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# this cell's metadata contains\n",
    "# \"nbsphinx\": \"hidden\" so it is hidden by nbsphinx\n",
    "\n",
    "\n",
    "def _set_paths() -> None:\n",
    "    # set the correct path when launched from within PyCharm\n",
    "\n",
    "    module_paths = [\"pytools\", \"facet\", \"sklearndf\"]\n",
    "\n",
    "    import sys\n",
    "    import os\n",
    "\n",
    "    if \"cwd\" not in globals():\n",
    "        # noinspection PyGlobalUndefined\n",
    "        global cwd\n",
    "        cwd = os.path.join(os.getcwd(), os.pardir, os.pardir, os.pardir)\n",
    "        os.chdir(cwd)\n",
    "    print(f\"working dir is '{os.getcwd()}'\")\n",
    "    for module_path in module_paths:\n",
    "        if module_path not in sys.path:\n",
    "            sys.path.insert(0, os.path.abspath(f\"{cwd}/{os.pardir}/{module_path}/src\"))\n",
    "        print(f\"added `{sys.path[0]}` to python paths\")\n",
    "\n",
    "\n",
    "def _ignore_warnings():\n",
    "    # ignore irrelevant warnings that would affect the output of this tutorial notebook\n",
    "\n",
    "    # ignore a useless LGBM warning\n",
    "    import warnings\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, message=r\".*Xcode_8\\.3\\.3\")\n",
    "\n",
    "\n",
    "_set_paths()\n",
    "_ignore_warnings()\n",
    "\n",
    "del _set_paths, _ignore_warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-28T14:29:36.537625Z",
     "start_time": "2020-08-28T14:29:36.419137Z"
    },
    "delete_for_interactive": true,
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# this cell's metadata contains\n",
    "# \"nbsphinx\": \"hidden\" so it is hidden by nbsphinx\n",
    "\n",
    "\n",
    "def _configure_matplotlib():\n",
    "    # set global options for matplotlib\n",
    "\n",
    "    import matplotlib\n",
    "\n",
    "    matplotlib.rcParams[\"figure.figsize\"] = (16.0, 8.0)\n",
    "    matplotlib.rcParams[\"figure.dpi\"] = 72\n",
    "\n",
    "\n",
    "_configure_matplotlib()\n",
    "\n",
    "del _configure_matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-27T13:10:25.031514Z",
     "start_time": "2020-08-27T13:10:25.029576Z"
    }
   },
   "source": [
    "# Required imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run this notebook, we will import not only the FACET package, but also other packages useful to solve this task. Overall, we can break down the imports into three categories: \n",
    "\n",
    "1. Common packages (pandas, matplotlib, etc.)\n",
    "2. Required FACET classes (inspection, selection, validation, simulation, etc.)\n",
    "3. Other BCG Gamma packages which simplify pipelining (sklearndf, see on [GitHub](https://github.com/BCG-Gamma/sklearndf/)) and support visualization (pytools, see on [GitHub](https://github.com/BCG-Gamma/pytools)) when using FACET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common package imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-28T14:29:37.518624Z",
     "start_time": "2020-08-28T14:29:36.540501Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import RepeatedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gamma FACET imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-28T14:29:38.013412Z",
     "start_time": "2020-08-28T14:29:37.520802Z"
    }
   },
   "outputs": [],
   "source": [
    "from facet.data import Sample\n",
    "from facet.crossfit import LearnerCrossfit\n",
    "from facet.inspection import LearnerInspector\n",
    "from facet.selection import LearnerRanker, LearnerGrid\n",
    "from facet.validation import BootstrapCV\n",
    "from facet.simulation.partition import ContinuousRangePartitioner\n",
    "from facet.simulation import UnivariateUpliftSimulator\n",
    "from facet.simulation.viz import SimulationDrawer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-18T14:15:20.686543Z",
     "start_time": "2020-08-18T14:15:20.683573Z"
    }
   },
   "source": [
    "**sklearndf imports**\n",
    "\n",
    "Instead of using the \"regular\" scikit-learn package, we are going to use sklearndf (see on [GitHub](https://github.com/BCG-Gamma/sklearndf/)). sklearndf is an open source library designed to address a common issue with scikit-learn: the outputs of transformers are numpy arrays, even when the input is a data frame. However, to inspect a model it is essential to keep track of the feature names. sklearndf retains all the functionality available through scikit-learn plus the feature traceability and usability associated with Pandas data frames. Additionally, the names of all your favourite scikit-learn functions are the same except for `DF` on the end. For example, the standard scikit-learn import:\n",
    "\n",
    "`from sklearn.pipeline import Pipeline`\n",
    "\n",
    "becomes:\n",
    "\n",
    "`from sklearndf.pipeline import PipelineDF`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-28T14:29:38.175010Z",
     "start_time": "2020-08-28T14:29:38.014809Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearndf.pipeline import PipelineDF, RegressorPipelineDF\n",
    "from sklearndf.regression import RandomForestRegressorDF\n",
    "from sklearndf.regression.extra import LGBMRegressorDF\n",
    "from sklearndf.transformation.extra import BorutaDF\n",
    "from sklearndf.transformation import SimpleImputerDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pytools imports**\n",
    "\n",
    "pytools (see on [GitHub](https://github.com/BCG-Gamma/pytools)) is an open source library containing general machine learning and visualization utilities, some of which are useful for visualising the advanced model inspection capabilities of FACET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-28T14:29:38.184383Z",
     "start_time": "2020-08-28T14:29:38.176913Z"
    }
   },
   "outputs": [],
   "source": [
    "from pytools.viz.dendrogram import DendrogramDrawer, DendrogramReportStyle\n",
    "from pytools.viz.distribution import ECDFDrawer\n",
    "from pytools.viz.matrix import MatrixDrawer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T10:40:59.547062Z",
     "start_time": "2020-08-05T10:40:59.544794Z"
    }
   },
   "source": [
    "# Preprocessing and initial feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to load our turbine decay data and create a simple preprocessing pipeline. For those interested initial EDA can be found in the Appendix [Exploratory Data Analysis](#Exploratory-Data-Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the prepared data frame\n",
    "decay_df = pd.read_csv(\n",
    "    \"sphinx/source/tutorial/gas_turbine_data.txt\", delim_whitespace=True\n",
    ")\n",
    "\n",
    "# assign human-readable labels\n",
    "decay_df.columns = [\n",
    "    \"Lever Position\",\n",
    "    \"Ship Speed\",\n",
    "    \"Turbine Shaft Torque (kN m)\",\n",
    "    \"Turbine Rate of Revolutions (rpm)\",\n",
    "    \"Generator Rate of Revolutions (rpm)\",\n",
    "    \"Starboard Propeller Torque (kN)\",\n",
    "    \"Port Propeller Torque (kN)\",\n",
    "    \"HP Turbine exit temp (C)\",\n",
    "    \"GT Compressor inlet air temp (C)\",\n",
    "    \"GT Compressor outlet air temp (C)\",\n",
    "    \"Turbine exit pressure (bar)\",\n",
    "    \"Compressor inlet air pressure (bar)\",\n",
    "    \"Compressor outlet air pressure (bar)\",\n",
    "    \"Turbine exhaust gas pressure (bar)\",\n",
    "    \"Turbine injection control\",\n",
    "    \"Fuel flow\",\n",
    "    \"GT Compressor decay state coeff\",\n",
    "    \"GT Turbine decay state coeff\",\n",
    "]\n",
    "\n",
    "# need to drop the other target\n",
    "decay_df.drop(\"GT Compressor decay state coeff\", inplace=True, axis=1)\n",
    "\n",
    "# to ensure quick running we will use a random sample of 2500 observations\n",
    "decay_df = decay_df.sample(n=2500, random_state=42)\n",
    "\n",
    "# have a look\n",
    "decay_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For easier data management we will create a sample object using FACET's `Sample` class, which allows us to: \n",
    "\n",
    "- Quickly access the target vs. features\n",
    "- Pass our data into sklearndf pipelines\n",
    "- Pass information to other FACET functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_obs = Sample(observations=decay_df, target_name=\"GT Turbine decay state coeff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we should create a minimum preprocessing pipeline. However, based on our EDA ([Exploratory Data Analysis](#Exploratory-Data-Analysis-(EDA)) ) we have no missing values or a need to manage categorical variables etc. However, while it is not needed we will create a simple imputation preprocessing pipeline using sklearndf's `SimpleImputerDF` to demonstrate how such a pipeline is created and included in subsequent analytic steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_preprocessing = PipelineDF(steps=[(\"impute\", SimpleImputerDF())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we perform some initial feature selection using Boruta, a recent approach shown to have quite good performance. The Boruta algorithm removes features that are no more predictive than random noise. If you are interested further, please see this  [article](https://www.jstatsoft.org/article/view/v036i11).\n",
    "\n",
    "The `BorutaDF` transformer in sklearndf package provides easy access to this powerful method. The approach relies on a tree-based learner, usually a random forest. For settings, a `max_depth` of between 3 and 7 is typically recommended, and here we rely on the default setting of 5. However, as this depends on the number of features and the complexity of interactions one could also explore the sensitivity of feature selection to this parameter. The number of trees is automatically managed by the Boruta feature selector argument `n_estimators=\"auto\"`.\n",
    "\n",
    "We also use parallelization for the random forest using `n_jobs` to accelerate the Boruta iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the Boruta object\n",
    "boruta_feat_select = BorutaDF(\n",
    "    estimator=RandomForestRegressorDF(max_depth=5, random_state=42, n_jobs=-3),\n",
    "    n_estimators=\"auto\",\n",
    "    max_iter=50,\n",
    "    random_state=42,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# combine Boruta with the preprocessing pipeline\n",
    "selection_pipeline = PipelineDF(\n",
    "    steps=[\n",
    "        (\"preprocessing\", feature_preprocessing),\n",
    "        (\"feature selection\", boruta_feat_select),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since [sklearndf](https://github.com/BCG-Gamma/sklearndf) closely follows the `scikit-learn` syntax, we can fit this pipeline on the `sample.features` and `sample.target` properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fit pipeline and print selected features\n",
    "selection_pipeline.fit(X=decay_obs.features, y=decay_obs.target)\n",
    "print(f\"Selected features: {selection_pipeline.feature_names_original_.unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-08-05T10:51:12.659Z"
    }
   },
   "source": [
    "Boruta selected 10 features which we will now keep in our FACET sample object. Note that this feature selection process could be included in a general preprocessing pipeline, however due to the computation involved, we have utilized Boruta here as an initial one-off processing step to narrow down the features for our classifier development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-28T14:32:16.589723Z",
     "start_time": "2020-08-28T14:32:16.585993Z"
    }
   },
   "outputs": [],
   "source": [
    "# update FACET sample object to only those features Boruta identified as useful\n",
    "decay_obs_initial_features = decay_obs.keep(\n",
    "    feature_names=selection_pipeline.feature_names_original_.unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting a learner using FACET ranker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FACET implements several additional useful wrappers which further simplify comparing and tuning a larger number of models and configurations: \n",
    "\n",
    "- `LearnerGrid`: allows you to pass a learner pipeline (i.e., regressor + any preprocessing) and a set of hyperparameters\n",
    "- `LearnerRanker`: multiple LearnerGrids can be passed into this class as a list - this allows tuning hyperparameters both across different types of learners in a single step and ranks the resulting models accordingly\n",
    "\n",
    "The following learners and hyperparameter ranges will be assessed using 10 repeated 5-fold cross-validation:\n",
    "\n",
    "\n",
    "1. **Random forest**: with hyperparameters\n",
    "    - min_samples_leaf: [8, 16]\n",
    "    - n_estimators: [20, 50, 100]  \n",
    "  \n",
    "  \n",
    "2. **Light gradient boosting**: with hyperparameters\n",
    "    - min_data_in_leaf: [8, 16]\n",
    "    - subsample: [0.8, 1]\n",
    "    - boosting_type: [\"gbdt\"]\n",
    "    \n",
    "Note if you want to see a list of hyperparameters you can use `regressor_name().get_params().keys()` where `regressor_name` could be for example `RandomForestRegressorDF` and if you want to see the default values, just use `regressor_name().get_params()`.\n",
    "\n",
    "Finally, for this exercise we will use the default performance metric for scoring and ranking our regressors, which is explained variance. Note that ranking uses the average performance minus two times the standard deviation, so that we consider both the average performance and variability when selecting a regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we specify the classifiers we want to train using `RegressorPipelineDF` from sklearndf. Note here we could also include any feature preprocessing steps by using the `preprocessing` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-28T14:32:16.599454Z",
     "start_time": "2020-08-28T14:32:16.596341Z"
    }
   },
   "outputs": [],
   "source": [
    "rforest_regressor = RegressorPipelineDF(\n",
    "    regressor=RandomForestRegressorDF(random_state=42),\n",
    ")\n",
    "\n",
    "lgbm_regressor = RegressorPipelineDF(\n",
    "    regressor=LGBMRegressorDF(random_state=42),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a list of learner grids where each learner grid is created using `LearnerGrid` and allows us to associate a `RegressorPipelineDF` with a specified set of hyperparameter via the `learner_parameters` argument. Note this structure allows us to easily include additional regressors and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-28T14:32:16.606273Z",
     "start_time": "2020-08-28T14:32:16.601757Z"
    }
   },
   "outputs": [],
   "source": [
    "regressor_grid = [\n",
    "    LearnerGrid(\n",
    "        pipeline=rforest_regressor,\n",
    "        learner_parameters={\"min_samples_leaf\": [8, 16], \"n_estimators\": [20, 50, 100]},\n",
    "    ),\n",
    "    LearnerGrid(\n",
    "        pipeline=lgbm_regressor,\n",
    "        learner_parameters={\n",
    "            \"min_data_in_leaf\": [8, 16],\n",
    "            \"subsample\": [0.8, 1],\n",
    "            \"boosting_type\": [\"gbdt\"],\n",
    "        },\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit the grid defined above using the `LeanerRanker`, which will run a gridsearch (or random search if defined) using 10 repeated 5-fold cross-validation on our selected set of features from Boruta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-28T14:33:24.495845Z",
     "start_time": "2020-08-28T14:32:16.609006Z"
    }
   },
   "outputs": [],
   "source": [
    "regressor_ranker = LearnerRanker(\n",
    "    grids=regressor_grid,\n",
    "    cv=RepeatedKFold(n_splits=5, n_repeats=10, random_state=42),\n",
    "    n_jobs=-3,\n",
    "    verbose=False,\n",
    "    scoring=\"explained_variance\",\n",
    ").fit(sample=decay_obs_initial_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-28T11:28:17.762068Z",
     "start_time": "2020-08-28T11:28:17.756762Z"
    }
   },
   "source": [
    "We can see how each model scored using the `summary_report()` method of the `LearnerRanker`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's look at performance for the top 5 ranked regressors\n",
    "regressor_ranker.summary_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see based on our learner ranker, we have selected a LGBM algorithm that achieved a mean explained variance of 0.96 with a SD of 0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T11:42:06.425585Z",
     "start_time": "2020-08-05T11:42:06.423740Z"
    }
   },
   "source": [
    "# Using the FACET inspector for model inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [SHAP approach](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions) has become the standard method for model inspection. SHAP values are used to explain the additive contribution of each feature to the prediction for a given observation. SHAP values are computed for every feature and observation.\n",
    "\n",
    "The FACET `LearnerInspector` computes SHAP values for each crossfit (i.e., a CV fold or bootstrap resample) using the best model identified by the `LearnerRanker`. The FACET `LearnerInspector` then provides advanced model inspection through new SHAP-based summary metrics for understanding feature redundancy and synergy. Redundancy and synergy are calculated using the new algorithm FACET implements for using SHAP values to understand model predictions.\n",
    "\n",
    "The definitions are as follows:\n",
    "\n",
    "- **Redundancy** represents how much information is shared between two features contributions to the model predictions. For example, temperature and pressure in a pressure cooker are redundant features for predicting cooking time since pressure will rise relative to the temperature, and vice versa. Therefore, knowing just one of either temperature or pressure will likely enable the same predictive accuracy. Redundancy is expressed as a percentage ranging from 0% (full uniqueness) to 100% (full redundancy).\n",
    "\n",
    "\n",
    "- **Synergy** represents how much the combined information of two features contributes to the model predictions. For example, given features X and Y as coordinates on a chess board, the colour of a square can only be predicted when considering X and Y in combination. Synergy is expressed as a percentage ranging from 0% (full autonomy) to 100% (full synergy).\n",
    "\n",
    "Both cases can apply at the same time, i.e. a pair of features can use some information synergistically while using other information redundantly.\n",
    "\n",
    "To analyse redundancy for all possible feature parings, the approach is:\n",
    "\n",
    "1.\tCalculate the feature redundancy matrix using SHAP value decomposition - this gives us pairwise redundancy between features, in the range of 0.0 (fully unique contributions) and 1.0 (fully redundant contributions)\n",
    "2.\tTransform the feature redundancy matrix into a feature distance matrix, where distance is expressed as (1.0 - redundancy)\n",
    "3.\tPerform hierarchical, single-linkage clustering on the distance matrix, thus identifying groups of low-distance, redundant features which activate “in tandem” to predict the outcome\n",
    "\n",
    "The same approach can be used to analyse synergy.\n",
    "\n",
    "The inspector can calculate all of this with a single method call, but also offers methods to access the intermediate results of each step. A lightweight visualization framework is available to render the results in different styles.\n",
    "\n",
    "SHAP values from the `LearnerInspector` can also be used with the SHAP package plotting functions for sample and observation level SHAP visualizations, such as SHAP distribution plots, dependency plots, force plots and waterfall plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-28T14:39:19.910709Z",
     "start_time": "2020-08-28T14:33:24.517489Z"
    }
   },
   "outputs": [],
   "source": [
    "# run inspector\n",
    "regression_inspector = LearnerInspector()\n",
    "regression_inspector.fit(\n",
    "    n_jobs=-3, verbose=False, crossfit=regressor_ranker.best_model_crossfit_\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-05T13:12:54.363503Z",
     "start_time": "2020-08-05T13:12:54.360977Z"
    }
   },
   "source": [
    "## Synergy and redundancy\n",
    "\n",
    "Synergy and redundancy are part of the key extensions FACET makes to using SHAP values to understand model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-28T16:08:11.655421Z",
     "start_time": "2020-08-28T16:08:11.057362Z"
    }
   },
   "outputs": [],
   "source": [
    "# synergy heatmaps\n",
    "synergy_matrix = regression_inspector.feature_synergy_matrix()\n",
    "MatrixDrawer(style=\"matplot%\").draw(synergy_matrix, title=\"Feature synergies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Synergy represents the degree to which one feature combines with another to generate a prediction.* We can see that the feature \"Compressor outlet air pressure (bar)\" is synergistic with other features. This means that the outlet air pressure **in combination** with rate of revolutions and generator rate of revolutions carries information for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-28T16:08:39.064822Z",
     "start_time": "2020-08-28T16:08:38.477297Z"
    }
   },
   "outputs": [],
   "source": [
    "# redundancy heatmap\n",
    "redundancy_matrix = regression_inspector.feature_redundancy_matrix()\n",
    "MatrixDrawer(style=\"matplot%\").draw(redundancy_matrix, title=\"Feature redundancies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Redundancy represents the shared information between two features.* We can see that this uncovers high redundancy between `Turbine injection control` and `Fuel flow`, as well as some redundancy for other feature pairs such as (`Turbine exhaust gas pressure` and `Turbine Rate of Revolutions`) and (`Turbine injection control` and `Compressor outlet air pressure`). Another way to look at redundancy is using a dendrogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-28T16:17:34.674348Z",
     "start_time": "2020-08-28T16:17:34.279844Z"
    }
   },
   "outputs": [],
   "source": [
    "# redundancy dendrogram\n",
    "redundancy_linkage = regression_inspector.feature_redundancy_linkage()\n",
    "DendrogramDrawer().draw(title=\"Redundancy linkage\", data=redundancy_linkage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience when working in a non-notebook environment, all of the `Drawer`s provided by the [pytools](https://github.com/BCG-Gamma/pytools) package also support a `style='text'` flag.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-28T16:25:17.524926Z",
     "start_time": "2020-08-28T16:25:17.520341Z"
    }
   },
   "outputs": [],
   "source": [
    "DendrogramDrawer(style=\"text\").draw(title=\"Redundancy linkage\", data=redundancy_linkage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the hierarchical clustering, we calculate a linkage tree and plug it into a dendrogram drawer. This makes it easy to visually single out features that are both important and mutually redundant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can we learn from the above? \n",
    "\n",
    "\n",
    "**Synergy**\n",
    "\n",
    "We can see in the synergy matrix that the feature \"Compressor outlet air pressure (bar)\" is synergistic with other features. This means that the outlet air pressure **in combination** with rate of revolutions and generator rate of revolutions carries information for the model. \n",
    "\n",
    "**Redundancy**\n",
    "\n",
    "The redundancy matrix and dendrogram reveals a \"cluster\" of two variables which are highly redundant - Turbine injection control and Fuel flow. That is, they provide the same information to the target and are likely dependent on each other. Looking at the process, the fuel flow is a consequence from the turbine injection control, so we can remove this redundant feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In turn, we remove the redundant features from the sample and create a revised sample on which we re-train the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-28T16:20:15.706573Z",
     "start_time": "2020-08-28T16:20:15.702905Z"
    }
   },
   "outputs": [],
   "source": [
    "redundant_features = [\"Fuel flow\", \"Turbine exhaust gas pressure (bar)\"]\n",
    "decay_obs_no_redundant = decay_obs_initial_features.drop(redundant_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-28T16:21:09.099135Z",
     "start_time": "2020-08-28T16:20:16.527678Z"
    }
   },
   "outputs": [],
   "source": [
    "# run the training pipeline again\n",
    "regressor_ranker = LearnerRanker(\n",
    "    grids=regressor_grid,\n",
    "    cv=RepeatedKFold(n_splits=5, n_repeats=10, random_state=42),\n",
    "    n_jobs=-3,\n",
    "    verbose=False,\n",
    ").fit(sample=decay_obs_no_redundant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check the best ranked model after removing redundant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-28T16:21:09.113777Z",
     "start_time": "2020-08-28T16:21:09.103236Z"
    }
   },
   "outputs": [],
   "source": [
    "regressor_ranker.best_model_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FACET univariate simulator: the impact of compressor outlet air temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-07T15:56:26.709687Z",
     "start_time": "2020-08-07T15:56:26.682253Z"
    }
   },
   "source": [
    "Another advantage of FACET is the ability to quickly instigate and run univariate simulations. Simulation enables us to gain insight into what value(s) of this compressor outlet air temp might minimize performance decay. Further, because FACET can use bootstrap cross validation, we can create a crossfit from our previous `LearnerRanker` best model to perform the simulation and to also quantify the uncertainty by using bootstrap confidence intervals.\n",
    "\n",
    "From the synergy matrix, we can see that the Compressor outlet temperature has the highest synergy with most other features. Therefore, we want to see how the target behaves if we simulate this feature such that each state had a constant outlet compressor temperature.\n",
    "\n",
    "The absolute SHAP values also confirm that the three features most synergistic with `GT Compressor outlet air temp (C)` are also the most important features according to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-28T16:30:48.931395Z",
     "start_time": "2020-08-28T16:30:48.866357Z"
    }
   },
   "outputs": [],
   "source": [
    "abs_shap_values = (\n",
    "    regression_inspector.shap_values()\n",
    "    .abs()\n",
    "    .sum(axis=0)\n",
    "    .reset_index()\n",
    "    .rename({0: \"Sum of abs SHAP values\"}, axis=1)\n",
    ")\n",
    "abs_shap_values.sort_values(by=\"Sum of abs SHAP values\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the basis for the simulation, we divide the feature into relevant partitions: \n",
    "\n",
    "- We use FACET's `ContinuousRangePartitioner` to split the range of observed values of the outlet air temperature into intervals of equal size. Each partition is represented by the central value of that partition. \n",
    "- For each partition, the simulator creates an artificial copy of the original sample assuming the variable to be simulated has the same value across all observations - which is the value representing the partition. Using the best `LearnerCrossfit` acquired from the ranker, the simulator now re-predicts all targets using the models trained for all folds and determines the average uplift of the target variable resulting from this.\n",
    "- The FACET `SimulationDrawer` allows us to visualise the result; both in a matplotlib and a plain-text style\n",
    "\n",
    "Finally, because FACET can use bootstrap cross validation, we can create a crossfit from our previous `LearnerRanker` best model to perform the simulation so we can quantify the uncertainty by using bootstrap confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boot_crossfit = LearnerCrossfit(\n",
    "    pipeline=regressor_ranker.best_model_.native_estimator,\n",
    "    cv=BootstrapCV(n_splits=250, random_state=42),\n",
    "    n_jobs=-3,\n",
    "    verbose=False,\n",
    ").fit(sample=decay_obs_no_redundant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-28T16:21:41.500250Z",
     "start_time": "2020-08-28T16:21:41.498208Z"
    }
   },
   "outputs": [],
   "source": [
    "# set-up and run a simulation\n",
    "SIM_FEATURE = \"GT Compressor outlet air temp (C)\"\n",
    "GTC_air_temp_simulator = UnivariateUpliftSimulator(crossfit=boot_crossfit, n_jobs=3)\n",
    "GTC_air_temp_partitions = ContinuousRangePartitioner()\n",
    "GTC_air_temp_simulation = GTC_air_temp_simulator.simulate_feature(\n",
    "    name=SIM_FEATURE, partitioner=GTC_air_temp_partitions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-28T16:21:09.250184Z",
     "start_time": "2020-08-28T16:21:09.246001Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualize the results\n",
    "SimulationDrawer().draw(data=GTC_air_temp_simulation, title=SIM_FEATURE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can also get a print out of simulation results\n",
    "SimulationDrawer(\"text\").draw(data=GTC_air_temp_simulation, title=SIM_FEATURE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from this that the turbine decay state coefficient is maximized when the outlet air temperature of the gas turbine is as small as possible. Note that this is only looking at the partitions of the historically observed range, as extrapolating these predictions into unobserved regions would risk creating infeasible scenarios. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "With the capabilities offered by FACET we were able to:\n",
    "\n",
    "1. Identify a learner using with good performance for predicting performance decay.\n",
    "2. Utilize advanced the SHAP value capabilities (synergy and redundancy) to identify additional features that could be removed (i.e., fuel flow and turbine exhaust gas pressure) and whether any features had strong synergistic effects - which in this case was primarily between turbine exhaust gas pressure and turbine rate of revolutions.\n",
    "3. Simulate the effect of changes in GT compressor outlet air temp on performance decay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data source\n",
    "\n",
    "**Dataset**\n",
    "\n",
    "The dataset used in this example is available on [Kaggle](https://www.kaggle.com/elikplim/maintenance-of-naval-propulsion-plants-data-set) and contains data from experiments carried out by means of a numerical simulator of a naval vessel (Frigate) characterized by a **Gas Turbine propulsion plant**. In this release of the simulator it is also possible to consider the performance decay over time of the GT components such as GT compressor and turbines.\n",
    "\n",
    "Each possible degradation state of the plant can be characterized by three parameters: \n",
    "\n",
    "- Ship speed (linear function of the lever position)\n",
    "- Compressor degradation coefficient kMc\n",
    "- Turbine degradation coefficient\n",
    "\n",
    "The degradation coefficients typically vary between [1; 0.95] for compressor and [1; 0.975] for the gas turbine. \n",
    "\n",
    "**Features**\n",
    "\n",
    "One observation in this dataset represents the current decay states of the compressor and the gas turbine along with several sensor readings of the shipping vessels at that point in time. \n",
    "\n",
    "Our target, the Gas Turbine decay  state is being modelled as a performance decay state metric which is measured as 1 to 0, 1 meaning delivering 100% of the nominal performance. Therefore, we want to determine the machine settings which **maximize the gas turbine decay state coefficient**. \n",
    "\n",
    "\n",
    "**Learning Problem**\n",
    "\n",
    "For this learning problem, we have three key objectives: \n",
    "\n",
    "1. Predict the decay state of the gas turbine as accurately as possible\n",
    "2. Understand which parameters drive the decay state of the turbine\n",
    "3. Analyse how these drivers interact with each other and the target\n",
    "\n",
    "\n",
    "While we can solve questions 1 and parts of question 2 with commonly used machine learning packages, `facet` will enable us to make better inferences about the way some of the features share or complement information and help us to figure out the optimal settings of the vessel to minimize the equipment degradation at a variety of ship speeds.\n",
    "\n",
    "Reference for the dataset used in this example is: A. Coraddu, L. Oneto, A. Ghio, S. Savio, D. Anguita, M. Figari, Machine Learning Approaches for Improving Condition? Based Maintenance of Naval Propulsion Plants, Journal of Engineering for the Maritime Environment, 2014, DOI: 10.1177/1475090214540874, (In Press)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_df = pd.read_csv(\n",
    "    \"sphinx/source/tutorial/gas_turbine_data.txt\", delim_whitespace=True\n",
    ")\n",
    "\n",
    "decay_df.columns = [\n",
    "    \"Lever Position\",\n",
    "    \"Ship Speed\",\n",
    "    \"Turbine Shaft Torque (kN m)\",\n",
    "    \"Turbine Rate of Revolutions (rpm)\",\n",
    "    \"Generator Rate of Revolutions (rpm)\",\n",
    "    \"Starboard Propeller Torque (kN)\",\n",
    "    \"Port Propeller Torque (kN)\",\n",
    "    \"HP Turbine exit temp (C)\",\n",
    "    \"GT Compressor inlet air temp (C)\",\n",
    "    \"GT Compressor outlet air temp (C)\",\n",
    "    \"Turbine exit pressure (bar)\",\n",
    "    \"Compressor inlet air pressure (bar)\",\n",
    "    \"Compressor outlet air pressure (bar)\",\n",
    "    \"Turbine exhaust gas pressure (bar)\",\n",
    "    \"Turbine injection control\",\n",
    "    \"Fuel flow\",\n",
    "    \"GT Compressor decay state coeff\",\n",
    "    \"GT Turbine decay state coeff\",\n",
    "]\n",
    "\n",
    "TARGET = \"GT Turbine decay state coeff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to drop the other target\n",
    "decay_df.drop(\"GT Compressor decay state coeff\", inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe the data\n",
    "decay_df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also quickly check for missing values, although we can see that there appears to be none."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count missing by feature\n",
    "decay_df.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use pytool's `ECDFDrawer()` to draw the cumulative distribution of the target. This shows us that the target is uniformly distributed in increments of 0.001 increments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the target distribution\n",
    "decay_obs = Sample(observations=decay_df, target_name=TARGET)\n",
    "ECDFDrawer().draw(decay_obs.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at feature distributions and correlations\n",
    "sns.pairplot(decay_obs.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick EDA summary:**\n",
    "\n",
    "- We have no missing values in our data\n",
    "- We are dealing with a linearly distributed target\n",
    "- The features exhibit a mixture of linear and non-linear relationships amongst each other. This gives us reason to test several non-parametric models and compare their performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facet-develop",
   "language": "python",
   "name": "facet-develop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
