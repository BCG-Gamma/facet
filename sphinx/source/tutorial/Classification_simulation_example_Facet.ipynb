{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "source": [
    "<img src=\"../_static/Gamma_Facet_Logo_RGB_LB.svg\" width=\"500\" style=\"padding-bottom: 70px; padding-top: 70px; margin: auto; display: block\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaining intuition about synergy and redundancy\n",
    "\n",
    "***\n",
    "\n",
    "**Robust and impactful Data Science with FACET**\n",
    "\n",
    "FACET enables us to perform several critical steps in best practice Data Science work flow easily, efficiently and reproducibly:\n",
    "\n",
    "1. Create a robust pipeline for learner selection using LearnerRanker and cross-validation.\n",
    "\n",
    "2. Enhance our model inspection to understand drivers of predictions using local explanations of features via [SHAP values](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions) by applying a novel methodology that decomposes SHAP values into measures of synergy, redundancy, and independence between each pair of features.\n",
    "\n",
    "3. Quickly apply historical simulation to gain key insights into feature values that minimize or maximize the predicted outcome.\n",
    "\n",
    "***\n",
    "\n",
    "**Context**\n",
    "\n",
    "With the advanced capabilities FACET provides by extending SHAP-based model inspection, it is important to gain some intuition for how the newly introduced measures for feature redundancy and synergy can vary. As SHAP values represent post-processing after data preparation, feature engineering, preprocessing and model selection/tuning, minimal simulation studies offer a way to make the connection as direct as possible.\n",
    "\n",
    "In this FACET tutorial we will conduct two simulation studies to gain intuition about synergy and redundancy:\n",
    "\n",
    "1. explore patterns in synergy and redundancy as a function of the individual and joint contribution of two continuous features in predicting a binary target where the features have varying degrees of correlation.\n",
    "2. explore how overfitting affects the accuracy of redundancy and synergy estimates for a random forest classifier by varying the `max_depth` parameter.\n",
    "\n",
    "***\n",
    "\n",
    "**Tutorial outline**\n",
    "\n",
    "1. [Required imports](#Required-imports)\n",
    "2. [Redundancy, Synergy and SHAP](#Redundancy,-Synergy-and-SHAP)\n",
    "3. [Data simulation](#Data-simulation)\n",
    "4. [How redundancy and synergy change with feature correlation and interaction](#How-redundancy-and-synergy-change-with-feature-correlation-and-interaction)\n",
    "5. [How overfitting affects the accuracy of redundancy and synergy estimates](#How-overfitting-affects-the-accuracy-of-redundancy-and-synergy-estimates)\n",
    "6. [Summary](#Summary)\n",
    "7. [What can you do next?](#What-can-you-do-next?)\n",
    "8. [Appendix](#Appendix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "delete_for_interactive": true,
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# this cell's metadata contains\n",
    "# \"nbsphinx\": \"hidden\" so it is hidden by nbsphinx\n",
    "\n",
    "\n",
    "def _set_paths() -> None:\n",
    "    # set the correct path when launched from within PyCharm\n",
    "\n",
    "    module_paths = [\"pytools\", \"facet\", \"sklearndf\"]\n",
    "\n",
    "    import sys\n",
    "    import os\n",
    "\n",
    "    if \"cwd\" not in globals():\n",
    "        # noinspection PyGlobalUndefined\n",
    "        global cwd\n",
    "        cwd = os.path.join(os.getcwd(), os.pardir, os.pardir, os.pardir)\n",
    "        os.chdir(cwd)\n",
    "    print(f\"working dir is '{os.getcwd()}'\")\n",
    "    for module_path in module_paths:\n",
    "        if module_path not in sys.path:\n",
    "            sys.path.insert(0, os.path.abspath(f\"{cwd}/{os.pardir}/{module_path}/src\"))\n",
    "        print(f\"added `{sys.path[0]}` to python paths\")\n",
    "\n",
    "\n",
    "def _ignore_warnings():\n",
    "    # ignore irrelevant warnings that would affect the output of this tutorial notebook\n",
    "\n",
    "    # ignore a useless LGBM warning\n",
    "    import warnings\n",
    "\n",
    "    warnings.filterwarnings(\"ignore\", category=UserWarning, message=r\".*Xcode_8\\.3\\.3\")\n",
    "\n",
    "\n",
    "_set_paths()\n",
    "_ignore_warnings()\n",
    "\n",
    "del _set_paths, _ignore_warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "delete_for_interactive": true,
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "# this cell's metadata contains\n",
    "# \"nbsphinx\": \"hidden\" so it is hidden by nbsphinx\n",
    "\n",
    "\n",
    "def _configure_matplotlib():\n",
    "    # set global options for matplotlib\n",
    "\n",
    "    import matplotlib\n",
    "\n",
    "    matplotlib.rcParams[\"figure.figsize\"] = (16.0, 8.0)\n",
    "    matplotlib.rcParams[\"figure.dpi\"] = 72\n",
    "\n",
    "\n",
    "_configure_matplotlib()\n",
    "\n",
    "del _configure_matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run this notebook, we will import not only the FACET package, but also other packages useful to solve this task. Overall, we can break down the imports into three categories: \n",
    "\n",
    "1. Common packages (pandas, matplotlib, etc.)\n",
    "2. Required FACET classes (inspection, selection, validation, simulation, etc.)\n",
    "3. sklearndf a BCG Gamma package that simplifies pipelining (see on [GitHub](https://github.com/orgs/BCG-Gamma/sklearndf/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Common package imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.linalg import toeplitz\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import RepeatedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gamma FACET imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facet.data import Sample\n",
    "from facet.crossfit import LearnerCrossfit\n",
    "from facet.inspection import LearnerInspector\n",
    "from facet.selection import LearnerRanker, LearnerGrid\n",
    "from facet.validation import BootstrapCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sklearndf imports**\n",
    "\n",
    "Instead of using the \"regular\" scikit-learn package, we are going to use sklearndf (see on [GitHub](https://github.com/orgs/BCG-Gamma/sklearndf/)). sklearndf is an open source library designed to address a common issue with scikit-learn: the outputs of transformers are numpy arrays, even when the input is a data frame. However, to inspect a model it is essential to keep track of the feature names. sklearndf retains all the functionality available through scikit-learn plus the feature traceability and usability associated with Pandas data frames. Additionally, the names of all your favourite scikit-learn functions are the same except for DF on the end. For example, the standard scikit-learn import:\n",
    "\n",
    "`from sklearn.pipeline import Pipeline`\n",
    "\n",
    "becomes:\n",
    "\n",
    "`from sklearndf.pipeline import PipelineDF`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearndf.pipeline import PipelineDF, ClassifierPipelineDF\n",
    "from sklearndf.classification import RandomForestClassifierDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redundancy, Synergy and SHAP\n",
    "\n",
    "Redundancy and synergy are part of the key extensions FACET makes to using SHAP values to understand model predictions. \n",
    "\n",
    "The [SHAP approach](http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions) has become the standard method for model inspection. SHAP values are used to explain the additive contribution of each feature to the prediction for a given observation. SHAP values are computed for every feature and observation.\n",
    "\n",
    "The FACET `LearnerInspector` computes SHAP values for each crossfit (i.e., a CV fold or bootstrap resample) using the best model identified by the `LearnerRanker`. The FACET `LearnerInspector` then provides advanced model inspection through new SHAP-based summary metrics for understanding feature redundancy and synergy.\n",
    "\n",
    "The definitions are as follows:\n",
    "\n",
    "- **Redundancy** represents how much information is shared between two features contributions to model predictions. For example, temperature and pressure in a pressure cooker are redundant features for predicting cooking time since pressure will rise relative to the temperature, and vice versa. Therefore, knowing just one of either temperature or pressure will likely enable the same predictive accuracy. Redundancy is expressed as a percentage ranging from 0% (full uniqueness) to 100% (full redundancy). \n",
    "\n",
    "\n",
    "- **Synergy** represents how much the combined information of two features contributes to the model predictions. For example, given features X and Y as coordinates on a chess board, the colour of a square can only be predicted when considering X and Y in combination. Synergy is expressed as a percentage ranging from 0% (full autonomy) to 100% (full synergy).\n",
    "\n",
    "In brief, redundancy represents the shared information between two features and synergy represents the degree to which one feature combines with another to generate a prediction. It is also important to recognize:\n",
    "\n",
    "- that any pair of features may have both redundancy and synergy\n",
    "- that SHAP values are dependent upon the model, so under or over fitting will influence redundancy and synergy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data simulation\n",
    "\n",
    "For the simulation studies we generate data as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ (X_1, X_2) \\sim N\\left[\\left(\\begin{array}{c} 0\\\\0 \\end{array}\\right), \\left(\\begin{array}{cc} 1 & \\rho\\\\ \\rho & 1 \\end{array}\\right)\\right]$$\n",
    "   \n",
    "    \n",
    "$$p = \\cfrac{1}{1 + exp(-[\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_1 X_2])}$$\n",
    "    \n",
    "    \n",
    "$$U \\sim \\textrm{U}(0,1)$$\n",
    "    \n",
    "    \n",
    "$$y = \\begin{cases}\n",
    "1 & U < p  \\\\\n",
    "0 & U \\geq p\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly we use the correlation ($\\rho$) between features to induce redundancy, and the balance between an interaction ($\\beta_3$) and main effects ($\\beta_1, \\beta_2$) to induce synergy. For example, as the correlation gets higher, we expect higher redundancy, and as the interaction gets stronger and the main effects get weaker, we expect higher synergy.\n",
    "\n",
    "The function used to simulate data according to the above specifications is `sim_interaction()` and can be found in the [Appendix](#Data-simulation-code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How redundancy and synergy change with feature correlation and interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this first simulation case study, we use the following parameters for data generation:\n",
    "\n",
    "- intercept ($\\beta_0$) = `[0]`\n",
    "- main effects ($\\beta_1, \\beta_2$) = `[0, 1, 2, 3]`\n",
    "- interaction ($\\beta_3$) = `[1, 2, 3]`\n",
    "- correlation ($\\rho$) = `[0, 0.2, 0.4, 0.6, 0.8]`\n",
    "\n",
    "For each combination of parameters above we simulate 20 datasets with 2000 observations. Model fitting is performed using 10 repeated 5-fold CV. The classifier used is a Random Forest with default hyperparameters.\n",
    "\n",
    "The code used to generate the data presented is shown in the [Appendix](#Simulation-study-1-code). You can experiment with the code and perform your own simulation studies, just be aware that it may take some time to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load simulation study data\n",
    "sns.set_style(\"darkgrid\")\n",
    "sim1_data = pd.read_csv(\"sphinx/source/tutorial/classification_sim1.csv\").set_index(\n",
    "    [\"main effects\", \"interaction\", \"correlation\"]\n",
    ")\n",
    "long_sim1_data = sim1_data[[\"redundancy\", \"synergy\"]].stack().reset_index()\n",
    "long_sim1_data.rename(\n",
    "    columns={\"level_3\": \"FACET metric\", 0: \"Redundany / Synergy\"}, inplace=True\n",
    ")\n",
    "\n",
    "# create summary plot\n",
    "sns.set_palette([\"#30c1d7\", \"#295e7e\"])\n",
    "g = sns.FacetGrid(\n",
    "    long_sim1_data,\n",
    "    col=\"main effects\",\n",
    "    row=\"interaction\",\n",
    "    hue=\"FACET metric\",\n",
    "    margin_titles=True,\n",
    ")\n",
    "g.map(\n",
    "    sns.lineplot,\n",
    "    \"correlation\",\n",
    "    \"Redundany / Synergy\",\n",
    "    estimator=\"mean\",\n",
    "    marker=\"o\",\n",
    "    ci=None,\n",
    ")\n",
    "g.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several interesting patterns we can observe from the plot:\n",
    "\n",
    "1. In general, the larger the interaction the higher the synergy.\n",
    "2. The greater the individual contributions of the features the lower the synergy.\n",
    "3. As the correlation increases, redundancy increases and synergy reduces.\n",
    "4. Redundancy increases with increasing individual contributions of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How overfitting affects the accuracy of redundancy and synergy estimates\n",
    "\n",
    "In this second simulation study we are going to explore the values of synergy and redundancy as a function of regularization in a random forest. The main regularization parameter is the `max_depth`, which controls the tree depth. In general, the deeper the tree the more likely we are to overfit the data. Because we will apply cross-validation we can get a sense of how model performance improves and then degrades with `max_depth` using the cross-validation curve.\n",
    "\n",
    "For this second simulation case study, we use the following parameters for data generation:\n",
    "\n",
    "- intercept ($\\beta_0$) = `[0]`\n",
    "- main effects ($\\beta_1, \\beta_2$) = `[1]`\n",
    "- interaction ($\\beta_3$) = `[3]`\n",
    "- correlation ($\\rho$) = `[0.5]`\n",
    "\n",
    "We simulate 20 datasets with 1000 observations. The classifier used is a Random Forest with default hyperparameters, except as follows:\n",
    "\n",
    "- `max_depth = [2, 4, 8, 16, 32]`\n",
    "- `n_estimators = [250]`\n",
    "\n",
    "For each combination of parameters above we perform model fitting using 10 repeated 5-fold CV for each of the 20 simulated datasets.\n",
    "\n",
    "The code used to generate the data presented is shown in the [Appendix](#Simulation-study-2-code). You can experiment with that code and perform your own simulation studies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load simulation study data\n",
    "val_curve_df = pd.read_csv(\"sphinx/source/tutorial/classification_sim2_cvcurve.csv\")\n",
    "sim2_data = pd.read_csv(\"sphinx/source/tutorial/classification_sim2.csv\").set_index(\n",
    "    [\"max_depth\", \"n_estimators\"]\n",
    ")\n",
    "long_sim2_data = sim2_data[[\"redundancy\", \"synergy\"]].stack().reset_index()\n",
    "long_sim2_data.rename(\n",
    "    columns={\"level_2\": \"FACET metric\", 0: \"Redundany / Synergy\"}, inplace=True\n",
    ")\n",
    "best_max_depth = val_curve_df.loc[val_curve_df[\"score\"].idxmax(), \"max_depth\"]\n",
    "\n",
    "# plot cross-validation curve from Learner Ranker\n",
    "plt.subplots(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "ax = sns.lineplot(\"max_depth\", \"score\", data=val_curve_df, marker=\"o\", color=\"black\")\n",
    "ax.set(xlabel=\"max_depth\", ylabel=\"mean CV AUC\", title=\"Cross-validation curve\")\n",
    "ax.axvline(best_max_depth, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "# create plot for redundancy and synergy as a function of max_depth\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.set_palette([\"#30c1d7\", \"#295e7e\"])\n",
    "ax = sns.lineplot(\n",
    "    \"max_depth\",\n",
    "    \"Redundany / Synergy\",\n",
    "    data=long_sim2_data,\n",
    "    hue=\"FACET metric\",\n",
    "    estimator=\"mean\",\n",
    "    marker=\"o\",\n",
    "    ci=None,\n",
    ")\n",
    "ax.set(title=\"Synergy and Redundancy as a function of max_depth\")\n",
    "ax.axvline(best_max_depth, color=\"red\", linestyle=\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe the following from the figure above:\n",
    "\n",
    "1. Based on the cross-validation curve, the best choice of `max_depth` is 6.\n",
    "2. The value of synergy at the best `max_depth` of 6 is around 47% which is lower than the largest estimate of 52% when we overfit (right of `max_depth` = 6), and much greater than the smallest estimate of 30% when we underfit (left of `max_depth` = 6).\n",
    "3. The value of redundancy at the best `max_depth` of 6 is around 27% which is higher than the smallest estimate of 23% when we overfit (right of `max_depth` = 6), and lower than the highest estimate of 30% when we underfit (left of `max_depth` = 6).\n",
    "\n",
    "This suggests for a pair of moderately correlated features with a moderate interaction and limited individual contributions, overfitting might cause us to over-estimate synergy (i.e., the model interprets noise as an interaction) and under-estimate redundancy, while underfitting can cause the opposite. As with all machine learning, identifying a well-tuned model is critical to obtaining appropriate estimates of synergy and redundancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "We conducted two simulation studies using a simple controlled setting where we knew the amount of correlation, individual and combined contributions to a binary target.\n",
    "\n",
    "- In the first simulation study we saw that the amount of correlation between two features as well as the strength of interaction and degree of independent contributions drives the balance between synergy and redundancy. \n",
    "- In the second simulation study we saw how both synergy and redundancy changed as a function of the `max_depth` parameter of our Random Forest classifier. For a pair of features with correlation and interaction, as `max_depth` increased synergy increased and redundancy decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What can you do next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several next steps that could be taken to gain further intuition regarding the capabilities of FACET:\n",
    "    \n",
    "1. Explore further values of main-effects, interactions and correlation between the two features used in the simulation studies.\n",
    "2. Add further features to the simulation and explore what happens when you have features that are correlated but only one contributes to prediction (i.e., a purely redundant feature).\n",
    "3. Try different learners and hyperparameters and see how the redundancy and synergy results change. Remember, the contributions of features to individual predictions is through the \"eyes\" of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data simulation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#```\n",
    "def sim_interaction(\n",
    "    n: int = 1000,\n",
    "    intercept: float = None,\n",
    "    coef_1: float = None,\n",
    "    coef_2: float = None,\n",
    "    coef_3: float = None,\n",
    "    corr: float = 0\n",
    "):\n",
    "\n",
    "    # two standard normal features for interaction term in the linear predictor\n",
    "    # mean and standard deviation of each feature\n",
    "    mu = [0, 0]\n",
    "    sd_mat = [1, 1]\n",
    "    \n",
    "    # correlation matrix\n",
    "    corr_mat = np.array([[1, corr], [corr, 1]])\n",
    "\n",
    "    # mean and standard deviation of each feature\n",
    "    mu = [0, 0]\n",
    "    sd_mat = [1, 1]\n",
    "\n",
    "    # calculate covariance\n",
    "    cov_mat = sd_mat*corr_mat*sd_mat\n",
    "    \n",
    "    tmp_data = pd.DataFrame(\n",
    "        np.random.multivariate_normal(mu, cov_mat, size=n),\n",
    "        columns=[\"feature_1\", \"feature_2\"],\n",
    "    )\n",
    "\n",
    "    # linear predictor\n",
    "    lp = (\n",
    "        intercept\n",
    "        + coef_1 * tmp_data.feature_1\n",
    "        + coef_2 * tmp_data.feature_2\n",
    "        + coef_3 * tmp_data.feature_1 * tmp_data.feature_2\n",
    "    )\n",
    "\n",
    "    # convert to probability\n",
    "    prob = 1 / (1 + np.exp(-lp))\n",
    "\n",
    "    # create target\n",
    "    tmp_data[\"target\"] = np.where(prob <= np.random.uniform(size=n), 0, 1)\n",
    "\n",
    "    return tmp_data\n",
    "#```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation study 1 code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#```\n",
    "# conditions to iterate over\n",
    "main_effects = [0, 1, 2, 3]\n",
    "interaction = [1, 2, 3]\n",
    "corr = [0, 0.2, 0.4, 0.6, 0.8]\n",
    "conditions = list(itertools.product(*[main_effects, interaction, corr]))\n",
    "n_sims = 20\n",
    "n_conditions = len(conditions)\n",
    "full_results = pd.DataFrame([])\n",
    "\n",
    "# iterate over conditions\n",
    "for i in range(n_conditions):\n",
    "    \n",
    "    print(i)\n",
    "    \n",
    "    print(i)\n",
    "    \n",
    "    # number of iterations for a set of conditions\n",
    "    for j in range(n_sims):\n",
    "        \n",
    "        print(j)\n",
    "        \n",
    "        # simulate data\n",
    "        sim_df = sim_interaction(intercept=0,\n",
    "                                 n=2000,\n",
    "                                 coef_1=conditions[i][0],\n",
    "                                 coef_2=conditions[i][0],\n",
    "                                 coef_3=conditions[i][1],\n",
    "                                 corr=conditions[i][2])\n",
    "        \n",
    "        # run crossfit\n",
    "        crossfit = LearnerCrossfit(\n",
    "            pipeline=ClassifierPipelineDF(classifier=RandomForestClassifierDF(random_state=42)),\n",
    "            cv=RepeatedKFold(n_splits=5, n_repeats=10, random_state=42),\n",
    "            n_jobs=-1,\n",
    "        ).fit(sample = Sample(\n",
    "            observations=sim_df,\n",
    "            feature_names=['feature_1', 'feature_2'],\n",
    "            target_name='target'\n",
    "        ))\n",
    "\n",
    "        # do a straight crossfit with fit inspector\n",
    "        inspector = LearnerInspector(n_jobs=-1).fit(crossfit=crossfit)\n",
    "\n",
    "        # obtain synergy and redundancy\n",
    "        redundancy_matrix = inspector.feature_redundancy_matrix()\n",
    "        synergy_matrix = inspector.feature_synergy_matrix()\n",
    "        \n",
    "        # assemble results\n",
    "        tmp_results = pd.Series({'coef_1': conditions[i][0],\n",
    "                   'coef_2': conditions[i][0],\n",
    "                   'interaction': conditions[i][1],\n",
    "                   'correlation': conditions[i][2],\n",
    "                   'redundancy': redundancy_matrix.loc['feature_1', 'feature_2'],\n",
    "                   'synergy': synergy_matrix.loc['feature_1', 'feature_2'],\n",
    "                   'y_mean': sim_df.target.mean()})\n",
    "        \n",
    "        full_results = full_results.append(tmp_results, ignore_index=True)\n",
    "        \n",
    "full_results['main effects'] = \"(\" + full_results['coef_1'].astype(str) + \", \" + full_results['coef_2'].astype(str) + \")\"\n",
    "\n",
    "# output to a csv file - and use in generating notebook\n",
    "full_results.to_csv('sphinx/source/tutorial/classification_sim1.csv', index=False)\n",
    "#```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation study 2 code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create a validation curve for `max_depth`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "```\n",
    "# hyperparameters to investigate\n",
    "max_depth = list(np.arange(2,33))\n",
    "n_estimators = [250]\n",
    "\n",
    "# simulate train data\n",
    "sim_df = sim_interaction(n=1000,\n",
    "                         intercept=0,\n",
    "                         coef_1=1,\n",
    "                         coef_2=1,\n",
    "                         coef_3=3,\n",
    "                         corr=0.5)\n",
    "\n",
    "# create classifier with required hyperparameters\n",
    "rf_pipeline = ClassifierPipelineDF(\n",
    "    classifier=RandomForestClassifierDF(random_state=42)\n",
    ")\n",
    "rf_grid = LearnerGrid(\n",
    "    pipeline=rf_pipeline,\n",
    "    learner_parameters={'max_depth': max_depth,\n",
    "                        'n_estimators': n_estimators}\n",
    ")\n",
    "\n",
    "# use learner ranker to assess hyperparameters to create a validation curve for max_depth\n",
    "ranker = LearnerRanker(\n",
    "    grids=[rf_grid],\n",
    "    cv=RepeatedKFold(n_splits=5, n_repeats=10, random_state=42),\n",
    "    n_jobs=-3,\n",
    "    scoring='roc_auc'\n",
    ").fit(sample = Sample(\n",
    "    observations=sim_df,\n",
    "    feature_names=['feature_1', 'feature_2'],\n",
    "    target_name='target')\n",
    ")\n",
    "\n",
    "# grab data for the plot\n",
    "val_df = pd.DataFrame(\n",
    "    data=[(evaluation.scores.mean(), evaluation.parameters['classifier__max_depth']) for evaluation in ranker.ranking_],\n",
    "    columns=[\"score\", \"max_depth\"]).sort_values(\n",
    "    by='max_depth'\n",
    ")\n",
    "\n",
    "# save dataset for plotting\n",
    "val_df.to_csv('sphinx/source/tutorial/classification_sim2_cvcurve.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we run a simulation to assess the change in synergy and redundancy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "```\n",
    "# hyperparameters to investigate\n",
    "max_depth = list(np.arange(2,33))\n",
    "n_estimators = [250]\n",
    "\n",
    "parm_grid = list(itertools.product(*[max_depth, n_estimators]))\n",
    "n_sims = 20\n",
    "n_parms = len(parm_grid)\n",
    "full_results = pd.DataFrame([])\n",
    "\n",
    "# number of iterations for a set of conditions\n",
    "for j in range(n_sims):\n",
    "    \n",
    "    print(j)\n",
    "        \n",
    "    # simulate train data\n",
    "    sim_df = sim_interaction(n=1000,\n",
    "                             intercept=0,\n",
    "                             coef_1=1,\n",
    "                             coef_2=1,\n",
    "                             coef_3=3,\n",
    "                             corr=0.5)\n",
    "    \n",
    "    # iterate over hyperparameters\n",
    "    for i in range(n_parms):\n",
    "        \n",
    "        print(i)\n",
    "        \n",
    "        # create classifier with required hyperparameters\n",
    "        clf = ClassifierPipelineDF(\n",
    "            classifier=RandomForestClassifierDF(\n",
    "                random_state=42,\n",
    "                max_depth=parm_grid[i][0],\n",
    "                n_estimators=parm_grid[i][1]\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # run crossfit\n",
    "        crossfit = LearnerCrossfit(\n",
    "            pipeline=clf,\n",
    "            cv=RepeatedKFold(n_splits=5, n_repeats=10, random_state=42),\n",
    "            n_jobs=-1,\n",
    "        ).fit(sample = Sample(\n",
    "            observations=sim_df,\n",
    "            feature_names=['feature_1', 'feature_2'],\n",
    "            target_name='target'\n",
    "        ))\n",
    "\n",
    "        # do a straight crossfit with fit inspector\n",
    "        inspector = LearnerInspector(n_jobs=-1).fit(crossfit=crossfit)\n",
    "\n",
    "        # obtain synergy and redundancy\n",
    "        redundancy_matrix = inspector.feature_redundancy_matrix()\n",
    "        synergy_matrix = inspector.feature_synergy_matrix()\n",
    "        \n",
    "        # assemble results\n",
    "        tmp_results = pd.Series({'max_depth': parm_grid[i][0],\n",
    "                   'n_estimators': parm_grid[i][1],\n",
    "                   'redundancy': redundancy_matrix.loc['feature_1', 'feature_2'],\n",
    "                   'synergy': synergy_matrix.loc['feature_1', 'feature_2'],\n",
    "                   'y_mean': sim_df.target.mean()})\n",
    "        \n",
    "        full_results = full_results.append(tmp_results, ignore_index=True)\n",
    "        \n",
    "# output to a csv file - and use in generating notebook\n",
    "full_results.to_csv('sphinx/source/tutorial/classification_sim2.csv', index=False)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "facet-develop",
   "language": "python",
   "name": "facet-develop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
